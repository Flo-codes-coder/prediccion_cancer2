{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-nPj_hpzOYo"
   },
   "source": [
    "## Cargar los Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2427,
     "status": "ok",
     "timestamp": 1719477082161,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "1ifwUaN4zOYr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar las tablas S4 y S6 del archivo Excel\n",
    "excel_file_path = 'Tables_S1_to_S11.xlsx'\n",
    "table_s4 = pd.read_excel(excel_file_path, sheet_name='Table S4')\n",
    "table_s6 = pd.read_excel(excel_file_path, sheet_name='Table S6')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OedXh4pzOYs"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC_n0u2szOYs"
   },
   "source": [
    "### Funciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1719477082161,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "t_qUsMW_zOYt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para limpiar cadenas de texto específicas en el DataFrame\n",
    "def clean_text(df, sequences_to_remove=['*', '**']):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            for sequence in sequences_to_remove:\n",
    "                df[column] = df[column].apply(lambda x: x.replace(sequence, '') if isinstance(x, str) and sequence in x else x)\n",
    "    return df\n",
    "\n",
    "# Función para convertir columnas a tipo numérico\n",
    "def convert_to_numeric(column):\n",
    "    if column.dtype in ['object', 'category']:\n",
    "        contains_letters = any(isinstance(val, str) and any(c.isalpha() for c in val) for val in column)\n",
    "        if not contains_letters:\n",
    "            return pd.to_numeric(column, errors='coerce')\n",
    "    return column\n",
    "\n",
    "# Función para cargar y limpiar datos\n",
    "def load_and_clean_data(file_path, sheet_name):\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    df = clean_text(df)\n",
    "    df = df.apply(convert_to_numeric)\n",
    "    return df\n",
    "\n",
    "# Función para obtener columnas numéricas\n",
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=[float, int]).columns.tolist()\n",
    "\n",
    "# Función para obtener columnas categóricas\n",
    "def get_categorical_columns(df):\n",
    "    return df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Función para generar la descripción estadística\n",
    "def descriptive_statistics(df):\n",
    "    return df.describe()\n",
    "\n",
    "# Función para graficar distribuciones\n",
    "def plot_distributions(df, numeric_columns):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(df[col].dropna(), bins=30, kde=True)\n",
    "        plt.title(f'Distribución de {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.show()\n",
    "\n",
    "# Función para graficar la distribución de una variable categórica\n",
    "def plot_categorical_distribution(df, categorical_column):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=categorical_column, data=df)\n",
    "    plt.title(f'Distribución de {categorical_column}')\n",
    "    plt.xlabel(categorical_column)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()\n",
    "\n",
    "# Función para graficar correlaciones\n",
    "def plot_correlations(df):\n",
    "    numeric_df = df.select_dtypes(include=[float, int])\n",
    "    correlation = numeric_df.corr()\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "    plt.title('Mapa de calor de correlaciones')\n",
    "    plt.show()\n",
    "\n",
    "# Función para comparar características por grupos\n",
    "def plot_group_comparisons(df, group_column, numeric_columns):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.boxplot(x=group_column, y=col, data=df)\n",
    "        plt.title(f'Boxplot de {col} por {group_column}')\n",
    "        plt.xlabel(group_column)\n",
    "        plt.ylabel(col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# Pipeline del EDA\n",
    "def eda_pipeline(file_path, sheet_name, group_column=None):\n",
    "    # Cargar y limpiar datos\n",
    "    df = load_and_clean_data(file_path, sheet_name)\n",
    "\n",
    "    # Obtener columnas numéricas y categóricas\n",
    "    numeric_columns = get_numeric_columns(df)\n",
    "    categorical_columns = get_categorical_columns(df)\n",
    "\n",
    "    # Descripción estadística\n",
    "    desc_stats = descriptive_statistics(df)\n",
    "    print(f\"Descripción Estadística de {sheet_name}:\\n\", desc_stats)\n",
    "\n",
    "    # Graficar distribuciones numéricas\n",
    "    plot_distributions(df, numeric_columns)\n",
    "\n",
    "    # Graficar distribuciones categóricas si existen\n",
    "    if categorical_columns:\n",
    "        for cat_col in categorical_columns:\n",
    "            plot_categorical_distribution(df, cat_col)\n",
    "\n",
    "    # Graficar correlaciones\n",
    "    plot_correlations(df)\n",
    "\n",
    "    # Comparar características por grupos\n",
    "    if group_column:\n",
    "        plot_group_comparisons(df, group_column, numeric_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l55Y79DWzOYt"
   },
   "source": [
    "### DF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1719477083832,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "4v6FGNMxzOYu"
   },
   "outputs": [],
   "source": [
    "# Variables para la ejecución\n",
    "file_path = '/content/Tables_S1_to_S11.xlsx'\n",
    "sheet_name_s4 = 'Table S4'\n",
    "sheet_name_s6 = 'Table S6'\n",
    "group_column = 'Tumor type'\n",
    "df_s4 = load_and_clean_data(file_path, sheet_name_s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1719477083832,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "haTz5SH1zOYu",
    "outputId": "54ef5ce6-f24a-4de2-f297-231152c67067"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d3f045ee-ac55-4ea6-9271-41fc5969162a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID #</th>\n",
       "      <th>Plasma sample ID #</th>\n",
       "      <th>Primary tumor sample ID #</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Tumor type</th>\n",
       "      <th>AJCC Stage</th>\n",
       "      <th>Histopathology</th>\n",
       "      <th>Plasma volume (mL)</th>\n",
       "      <th>Plasma DNA concentration (ng/mL)</th>\n",
       "      <th>CancerSEEK Logistic Regression Score</th>\n",
       "      <th>CancerSEEK Test Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRC 455</td>\n",
       "      <td>CRC 455 PLS 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>59.811088</td>\n",
       "      <td>Male</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Colorectum</td>\n",
       "      <td>I</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.079696</td>\n",
       "      <td>0.938342</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRC 456</td>\n",
       "      <td>CRC 456 PLS 1</td>\n",
       "      <td>CRC 456 PT1</td>\n",
       "      <td>59.091034</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Colorectum</td>\n",
       "      <td>I</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46.005220</td>\n",
       "      <td>0.925363</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CRC 457</td>\n",
       "      <td>CRC 457 PLS 1</td>\n",
       "      <td>CRC 457 PT1</td>\n",
       "      <td>68.618754</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Colorectum</td>\n",
       "      <td>II</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.940071</td>\n",
       "      <td>0.852367</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRC 458</td>\n",
       "      <td>CRC 458 PLS 1</td>\n",
       "      <td>CRC 458 PT1</td>\n",
       "      <td>69.563313</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Colorectum</td>\n",
       "      <td>II</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.149544</td>\n",
       "      <td>0.617639</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CRC 459</td>\n",
       "      <td>CRC 459 PLS 1</td>\n",
       "      <td>CRC 459 PT1</td>\n",
       "      <td>43.359343</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Colorectum</td>\n",
       "      <td>II</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.814674</td>\n",
       "      <td>0.318434</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>PAPA 1353</td>\n",
       "      <td>PAPA 1353 PLS 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Ovary</td>\n",
       "      <td>I</td>\n",
       "      <td>Epithelial carcinoma</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.546670</td>\n",
       "      <td>0.980312</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>PAPA 1354</td>\n",
       "      <td>PAPA 1354 PLS 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Ovary</td>\n",
       "      <td>I</td>\n",
       "      <td>Epithelial carcinoma</td>\n",
       "      <td>3.5</td>\n",
       "      <td>22.834150</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>PAPA 1355</td>\n",
       "      <td>PAPA 1355 PLS 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Ovary</td>\n",
       "      <td>III</td>\n",
       "      <td>Epithelial carcinoma</td>\n",
       "      <td>3.5</td>\n",
       "      <td>64.506838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>PAPA 1356</td>\n",
       "      <td>PAPA 1356 PLS 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Ovary</td>\n",
       "      <td>II</td>\n",
       "      <td>Epithelial carcinoma</td>\n",
       "      <td>3.5</td>\n",
       "      <td>13.709668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>PAPA 1357</td>\n",
       "      <td>PAPA 1357 PLS 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Ovary</td>\n",
       "      <td>III</td>\n",
       "      <td>Epithelial carcinoma</td>\n",
       "      <td>3.5</td>\n",
       "      <td>19.808656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1817 rows × 13 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3f045ee-ac55-4ea6-9271-41fc5969162a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d3f045ee-ac55-4ea6-9271-41fc5969162a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d3f045ee-ac55-4ea6-9271-41fc5969162a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-59e0cc76-e926-4567-99df-95fc2b853c8e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59e0cc76-e926-4567-99df-95fc2b853c8e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-59e0cc76-e926-4567-99df-95fc2b853c8e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     Patient ID # Plasma sample ID # Primary tumor sample ID #        Age  \\\n",
       "0         CRC 455      CRC 455 PLS 1             Not available  59.811088   \n",
       "1         CRC 456      CRC 456 PLS 1               CRC 456 PT1  59.091034   \n",
       "2         CRC 457      CRC 457 PLS 1               CRC 457 PT1  68.618754   \n",
       "3         CRC 458      CRC 458 PLS 1               CRC 458 PT1  69.563313   \n",
       "4         CRC 459      CRC 459 PLS 1               CRC 459 PT1  43.359343   \n",
       "...           ...                ...                       ...        ...   \n",
       "1812    PAPA 1353    PAPA 1353 PLS 1             Not available  55.000000   \n",
       "1813    PAPA 1354    PAPA 1354 PLS 1             Not available  57.000000   \n",
       "1814    PAPA 1355    PAPA 1355 PLS 1             Not available  60.000000   \n",
       "1815    PAPA 1356    PAPA 1356 PLS 1             Not available  49.000000   \n",
       "1816    PAPA 1357    PAPA 1357 PLS 1             Not available  60.000000   \n",
       "\n",
       "         Sex       Race  Tumor type AJCC Stage        Histopathology  \\\n",
       "0       Male  Caucasian  Colorectum          I        Adenocarcinoma   \n",
       "1     Female  Caucasian  Colorectum          I        Adenocarcinoma   \n",
       "2     Female  Caucasian  Colorectum         II        Adenocarcinoma   \n",
       "3     Female  Caucasian  Colorectum         II        Adenocarcinoma   \n",
       "4     Female  Caucasian  Colorectum         II        Adenocarcinoma   \n",
       "...      ...        ...         ...        ...                   ...   \n",
       "1812  Female  Caucasian       Ovary          I  Epithelial carcinoma   \n",
       "1813  Female  Caucasian       Ovary          I  Epithelial carcinoma   \n",
       "1814  Female  Caucasian       Ovary        III  Epithelial carcinoma   \n",
       "1815  Female  Caucasian       Ovary         II  Epithelial carcinoma   \n",
       "1816  Female      Black       Ovary        III  Epithelial carcinoma   \n",
       "\n",
       "      Plasma volume (mL)  Plasma DNA concentration (ng/mL)  \\\n",
       "0                    5.0                          6.079696   \n",
       "1                    4.0                         46.005220   \n",
       "2                    4.5                          6.940071   \n",
       "3                    7.5                          7.149544   \n",
       "4                    5.0                          9.814674   \n",
       "...                  ...                               ...   \n",
       "1812                 3.5                          6.546670   \n",
       "1813                 3.5                         22.834150   \n",
       "1814                 3.5                         64.506838   \n",
       "1815                 3.5                         13.709668   \n",
       "1816                 3.5                         19.808656   \n",
       "\n",
       "      CancerSEEK Logistic Regression Score CancerSEEK Test Result  \n",
       "0                                 0.938342               Positive  \n",
       "1                                 0.925363               Positive  \n",
       "2                                 0.852367               Negative  \n",
       "3                                 0.617639               Negative  \n",
       "4                                 0.318434               Negative  \n",
       "...                                    ...                    ...  \n",
       "1812                              0.980312               Positive  \n",
       "1813                              0.999995               Positive  \n",
       "1814                              1.000000               Positive  \n",
       "1815                              1.000000               Positive  \n",
       "1816                              1.000000               Positive  \n",
       "\n",
       "[1817 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMlXjcvXzOYv"
   },
   "source": [
    "### DF6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1719477084873,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "BNsVtl14zOYv"
   },
   "outputs": [],
   "source": [
    "df_s6 = load_and_clean_data(file_path, sheet_name_s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719477084874,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "g8GOc8HMzOYw",
    "outputId": "d32263e5-93c7-4ebe-c6d2-fb5f89b9ef7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1817 entries, 0 to 1816\n",
      "Data columns (total 47 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   Patient ID #                          1817 non-null   object \n",
      " 1   Sample ID #                           1817 non-null   object \n",
      " 2   Tumor type                            1817 non-null   object \n",
      " 3   AJCC Stage                            1005 non-null   object \n",
      " 4   AFP (pg/ml)                           1817 non-null   float64\n",
      " 5   Angiopoietin-2 (pg/ml)                1817 non-null   float64\n",
      " 6   AXL (pg/ml)                           1811 non-null   float64\n",
      " 7   CA-125 (U/ml)                         1817 non-null   float64\n",
      " 8   CA 15-3 (U/ml)                        1817 non-null   float64\n",
      " 9   CA19-9 (U/ml)                         1817 non-null   float64\n",
      " 10  CD44 (ng/ml)                          1811 non-null   float64\n",
      " 11  CEA (pg/ml)                           1817 non-null   float64\n",
      " 12  CYFRA 21-1 (pg/ml)                    1817 non-null   float64\n",
      " 13  DKK1 (ng/ml)                          1817 non-null   float64\n",
      " 14  Endoglin (pg/ml)                      1817 non-null   float64\n",
      " 15  FGF2 (pg/ml)                          1817 non-null   float64\n",
      " 16  Follistatin (pg/ml)                   1817 non-null   float64\n",
      " 17  Galectin-3 (ng/ml)                    1817 non-null   float64\n",
      " 18  G-CSF (pg/ml)                         1810 non-null   float64\n",
      " 19  GDF15 (ng/ml)                         1817 non-null   float64\n",
      " 20  HE4 (pg/ml)                           1817 non-null   float64\n",
      " 21  HGF (pg/ml)                           1817 non-null   float64\n",
      " 22  IL-6 (pg/ml)                          1817 non-null   float64\n",
      " 23  IL-8 (pg/ml)                          1817 non-null   float64\n",
      " 24  Kallikrein-6 (pg/ml)                  1811 non-null   float64\n",
      " 25  Leptin (pg/ml)                        1817 non-null   float64\n",
      " 26  Mesothelin (ng/ml)                    1811 non-null   float64\n",
      " 27  Midkine (pg/ml)                       1811 non-null   float64\n",
      " 28  Myeloperoxidase (ng/ml)               1817 non-null   float64\n",
      " 29  NSE (ng/ml)                           1817 non-null   float64\n",
      " 30  OPG (ng/ml)                           1817 non-null   float64\n",
      " 31  OPN (pg/ml)                           1817 non-null   float64\n",
      " 32  PAR (pg/ml)                           1811 non-null   float64\n",
      " 33  Prolactin (pg/ml)                     1817 non-null   float64\n",
      " 34  sEGFR (pg/ml)                         1811 non-null   float64\n",
      " 35  sFas (pg/ml)                          1816 non-null   float64\n",
      " 36  SHBG (nM)                             1817 non-null   float64\n",
      " 37  sHER2/sEGFR2/sErbB2 (pg/ml)           1811 non-null   float64\n",
      " 38  sPECAM-1 (pg/ml)                      1811 non-null   float64\n",
      " 39  TGFa (pg/ml)                          1817 non-null   float64\n",
      " 40  Thrombospondin-2 (pg/ml)              1811 non-null   float64\n",
      " 41  TIMP-1 (pg/ml)                        1817 non-null   float64\n",
      " 42  TIMP-2 (pg/ml)                        1817 non-null   float64\n",
      " 43  CancerSEEK Logistic Regression Score  1817 non-null   float64\n",
      " 44  CancerSEEK Test Result                1817 non-null   object \n",
      " 45  Omega score                           1751 non-null   float64\n",
      " 46  Sex                                   1817 non-null   object \n",
      "dtypes: float64(41), object(6)\n",
      "memory usage: 667.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_s6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1640,
     "status": "ok",
     "timestamp": 1719477086510,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "SDPxF8L2zOYw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Crear una carpeta para guardar los resultados si no existe\n",
    "os.makedirs('resultados', exist_ok=True)\n",
    "\n",
    "# Función para limpiar cadenas de texto específicas en el DataFrame\n",
    "def clean_text(df, sequences_to_remove=['*', '**']):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            for sequence in sequences_to_remove:\n",
    "                df[column] = df[column].apply(lambda x: x.replace(sequence, '') if isinstance(x, str) and sequence in x else x)\n",
    "    return df\n",
    "\n",
    "# Función para convertir columnas a tipo numérico\n",
    "def convert_to_numeric(column):\n",
    "    if column.dtype in ['object', 'category']:\n",
    "        contains_letters = any(isinstance(val, str) and any(c.isalpha() for c in val) for val in column)\n",
    "        if not contains_letters:\n",
    "            return pd.to_numeric(column, errors='coerce')\n",
    "    return column\n",
    "\n",
    "# Función para cargar y limpiar datos\n",
    "def load_and_clean_data(file_path, sheet_name):\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    df = clean_text(df)\n",
    "    df = df.apply(convert_to_numeric)\n",
    "    return df\n",
    "\n",
    "# Función para obtener columnas numéricas\n",
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=[float, int]).columns.tolist()\n",
    "\n",
    "# Función para obtener columnas categóricas\n",
    "def get_categorical_columns(df):\n",
    "    return df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Función para limpiar nombres de archivos\n",
    "def clean_filename(filename):\n",
    "    return filename.replace('/', '_').replace('\\\\', '_')\n",
    "\n",
    "# Función para generar la descripción estadística\n",
    "def descriptive_statistics(df, sheet_name):\n",
    "    desc_stats = df.describe()\n",
    "    desc_stats.to_csv(f'resultados/{clean_filename(sheet_name)}_descriptive_statistics.csv')\n",
    "    return desc_stats\n",
    "\n",
    "# Función para graficar distribuciones\n",
    "def plot_distributions(df, numeric_columns, sheet_name):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(df[col].dropna(), bins=30, kde=True)\n",
    "        plt.title(f'Distribución de {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.savefig(f'resultados/{clean_filename(sheet_name)}_distribution_{clean_filename(col)}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Función para graficar la distribución de una variable categórica\n",
    "def plot_categorical_distribution(df, categorical_column, sheet_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=categorical_column, data=df)\n",
    "    plt.title(f'Distribución de {categorical_column}')\n",
    "    plt.xlabel(categorical_column)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.savefig(f'resultados/{clean_filename(sheet_name)}_categorical_distribution_{clean_filename(categorical_column)}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Función para graficar correlaciones\n",
    "def plot_correlations(df, sheet_name):\n",
    "    numeric_df = df.select_dtypes(include=[float, int])\n",
    "    correlation = numeric_df.corr()\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "    plt.title('Mapa de calor de correlaciones')\n",
    "    plt.savefig(f'resultados/{clean_filename(sheet_name)}_correlations_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Función para comparar características por grupos\n",
    "def plot_group_comparisons(df, group_column, numeric_columns, sheet_name):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.boxplot(x=group_column, y=col, data=df)\n",
    "        plt.title(f'Boxplot de {col} por {group_column}')\n",
    "        plt.xlabel(group_column)\n",
    "        plt.ylabel(col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(f'resultados/{clean_filename(sheet_name)}_group_comparison_{clean_filename(col)}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Función para analizar valores faltantes\n",
    "def analyze_missing_values(df, sheet_name):\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "    missing_values.to_csv(f'resultados/{clean_filename(sheet_name)}_missing_values.csv')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Mapa de valores faltantes')\n",
    "    plt.savefig(f'resultados/{clean_filename(sheet_name)}_missing_values_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Función para realizar imputación de valores faltantes\n",
    "def impute_missing_values(df, strategy='mean'):\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    numeric_columns = get_numeric_columns(df)\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    return df\n",
    "\n",
    "# Función para realizar Análisis de Componentes Principales (PCA)\n",
    "def perform_pca(df, sheet_name, n_components=2):\n",
    "    numeric_df = df.select_dtypes(include=[float, int]).dropna()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(numeric_df)\n",
    "    principal_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    principal_df.to_csv(f'resultados/{clean_filename(sheet_name)}_pca.csv')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='PC1', y='PC2', data=principal_df)\n",
    "    plt.title('Análisis de Componentes Principales (PCA)')\n",
    "    plt.savefig(f'resultados/{clean_filename(sheet_name)}_pca.png')\n",
    "    plt.close()\n",
    "\n",
    "# Pipeline del EDA\n",
    "def eda_pipeline(file_path, sheet_name, group_column=None):\n",
    "    # Cargar y limpiar datos\n",
    "    df = load_and_clean_data(file_path, sheet_name)\n",
    "\n",
    "    # Análisis de valores faltantes\n",
    "    analyze_missing_values(df, sheet_name)\n",
    "\n",
    "    # Imputación de valores faltantes\n",
    "    df = impute_missing_values(df)\n",
    "\n",
    "    # Obtener columnas numéricas y categóricas\n",
    "    numeric_columns = get_numeric_columns(df)\n",
    "    categorical_columns = get_categorical_columns(df)\n",
    "\n",
    "    # Descripción estadística\n",
    "    desc_stats = descriptive_statistics(df, sheet_name)\n",
    "    print(f\"Descripción Estadística de {sheet_name}:\\n\", desc_stats)\n",
    "\n",
    "    # Graficar distribuciones numéricas\n",
    "    plot_distributions(df, numeric_columns, sheet_name)\n",
    "\n",
    "    # Graficar distribuciones categóricas si existen\n",
    "    if categorical_columns:\n",
    "        for cat_col in categorical_columns:\n",
    "            plot_categorical_distribution(df, cat_col, sheet_name)\n",
    "\n",
    "    # Graficar correlaciones\n",
    "    plot_correlations(df, sheet_name)\n",
    "\n",
    "    # Comparar características por grupos\n",
    "    if group_column:\n",
    "        plot_group_comparisons(df, group_column, numeric_columns, sheet_name)\n",
    "\n",
    "    # Análisis de Componentes Principales (PCA)\n",
    "    perform_pca(df, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaFsggWWzOYx"
   },
   "source": [
    "# EDA S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40426,
     "status": "ok",
     "timestamp": 1719477126934,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "gpiE41sfzOYx",
    "outputId": "a75cf092-195a-481b-be91-be78cc76a260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descripción Estadística de Table S4:\n",
      "                Age  Plasma volume (mL)  Plasma DNA concentration (ng/mL)  \\\n",
      "count  1817.000000         1817.000000                       1817.000000   \n",
      "mean     56.808473            7.369356                          8.924964   \n",
      "std      17.311677            0.623067                         15.176272   \n",
      "min      17.000000            2.000000                          0.001088   \n",
      "25%      47.405886            7.500000                          2.313844   \n",
      "50%      60.000000            7.500000                          4.376189   \n",
      "75%      69.427498            7.500000                          8.195708   \n",
      "max      93.000000            7.500000                        157.476775   \n",
      "\n",
      "       CancerSEEK Logistic Regression Score  \n",
      "count                           1817.000000  \n",
      "mean                               0.552796  \n",
      "std                                0.365760  \n",
      "min                                0.062389  \n",
      "25%                                0.198374  \n",
      "50%                                0.460998  \n",
      "75%                                0.989342  \n",
      "max                                1.000000  \n"
     ]
    }
   ],
   "source": [
    "eda_pipeline(file_path, sheet_name_s4, group_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuHfzu-XzOYx"
   },
   "source": [
    "# EDA S6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58982,
     "status": "ok",
     "timestamp": 1719477185902,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "N4LOt_N7zOYx",
    "outputId": "0ac2f2f0-1dd9-43c1-bd22-e2488169d418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descripción Estadística de Table S6:\n",
      "          AFP (pg/ml)  Angiopoietin-2 (pg/ml)   AXL (pg/ml)  CA-125 (U/ml)  \\\n",
      "count    1817.000000             1817.000000   1817.000000    1817.000000   \n",
      "mean     7109.350915             1908.423015   2367.282468      25.183043   \n",
      "std     52353.921638             1814.768306   1367.437976     184.585378   \n",
      "min       706.158000               38.391000    109.440000       4.608000   \n",
      "25%       829.980000              997.490000   1483.010000       4.890000   \n",
      "50%       946.938000             1498.920000   2138.070000       4.980000   \n",
      "75%      1848.540000             2259.460000   2927.800000       6.400000   \n",
      "max    600608.892000            30001.791000  12247.310000    3600.024000   \n",
      "\n",
      "       CA 15-3 (U/ml)  CA19-9 (U/ml)  CD44 (ng/ml)    CEA (pg/ml)  \\\n",
      "count     1817.000000    1817.000000   1817.000000    1817.000000   \n",
      "mean        20.628611      53.828772     19.533026    4427.203445   \n",
      "std         64.345361     409.030952     11.322784   23696.803234   \n",
      "min          1.320000      14.214000      6.750000     426.438000   \n",
      "25%          7.110000      16.320000     11.970000     614.200000   \n",
      "50%         12.180000      16.482000     16.800000    1045.440000   \n",
      "75%         19.840000      18.600000     23.760000    1924.610000   \n",
      "max       1177.446000   12491.472000    148.440000  337245.426000   \n",
      "\n",
      "       CYFRA 21-1 (pg/ml)  DKK1 (ng/ml)  ...  sFas (pg/ml)    SHBG (nM)  \\\n",
      "count        1.817000e+03   1817.000000  ...   1817.000000  1817.000000   \n",
      "mean         4.843461e+03      1.050809  ...   1390.840456    67.922256   \n",
      "std          4.238234e+04      0.442416  ...   2354.152106    54.478992   \n",
      "min          1.816458e+03      0.350000  ...    192.948000     1.500000   \n",
      "25%          1.955244e+03      0.740000  ...    206.334000    31.430000   \n",
      "50%          1.994874e+03      0.940000  ...   1126.650000    53.350000   \n",
      "75%          2.106970e+03      1.250000  ...   1802.510000    87.350000   \n",
      "max          1.475728e+06      5.970000  ...  61146.100000   478.840000   \n",
      "\n",
      "       sHER2/sEGFR2/sErbB2 (pg/ml)  sPECAM-1 (pg/ml)  TGFa (pg/ml)  \\\n",
      "count                  1817.000000       1817.000000   1817.000000   \n",
      "mean                   5765.088272       5883.915544     28.108763   \n",
      "std                    4368.986379       2170.688976    283.264077   \n",
      "min                     306.280000        219.830000     15.258000   \n",
      "25%                    4232.490000       4388.270000     16.200000   \n",
      "50%                    5269.210000       5511.970000     16.488000   \n",
      "75%                    6465.230000       7019.320000     16.698000   \n",
      "max                  150848.100000      20178.170000  12018.864000   \n",
      "\n",
      "       Thrombospondin-2 (pg/ml)  TIMP-1 (pg/ml)  TIMP-2 (pg/ml)  \\\n",
      "count               1817.000000     1817.000000     1817.000000   \n",
      "mean                5502.450456    70058.422889    40261.117001   \n",
      "std                10187.250364    47577.490820    12970.481941   \n",
      "min                  482.140000      976.550000    15026.320000   \n",
      "25%                 1174.650000    41231.360000    30752.350000   \n",
      "50%                 2245.650000    59282.780000    37735.410000   \n",
      "75%                 5666.720000    82928.930000    46794.540000   \n",
      "max               157461.070000   569512.690000   105748.640000   \n",
      "\n",
      "       CancerSEEK Logistic Regression Score  Omega score  \n",
      "count                           1817.000000  1817.000000  \n",
      "mean                               0.552796     4.439652  \n",
      "std                                0.365760    20.773534  \n",
      "min                                0.062389     0.000000  \n",
      "25%                                0.198374     0.722036  \n",
      "50%                                0.460998     0.981666  \n",
      "75%                                0.989342     1.471243  \n",
      "max                                1.000000   333.234911  \n",
      "\n",
      "[8 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "eda_pipeline(file_path, sheet_name_s6, group_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVynT1DKzOYy"
   },
   "source": [
    "### Interpretacion tabla S4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdqae69OzOYy"
   },
   "source": [
    "* Balance de Datos: Hay un desbalance significativo en algunas categorías (por ejemplo, Race, Tumor type, y CancerSEEK Test Result).\n",
    "* Distribuciones: Algunas variables muestran distribuciones sesgadas (por ejemplo, Plasma DNA concentration), lo que puede requerir transformaciones para un análisis adecuado.\n",
    "* Correlaciones: Identificamos correlaciones moderadas entre algunas variables clave, lo que puede guiar el desarrollo de modelos predictivos.\n",
    "* PCA: Indica alta variabilidad y dispersión en los datos, lo que sugiere que múltiples factores contribuyen a las diferencias observadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf6Gm72UzOYy"
   },
   "source": [
    "### Interpretacion tabla s6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jexvmLNXzOYy"
   },
   "source": [
    "El análisis exploratorio de los datos (EDA) de la tabla S6 revela varias observaciones importantes:\n",
    "\n",
    "Variabilidad entre tipos de tumores: Muchos biomarcadores muestran variaciones significativas entre diferentes tipos de tumores, lo que puede ser útil para la clasificación y predicción del tipo de cáncer.\n",
    "\n",
    "Valores atípicos: Existen varios valores atípicos en los datos, lo que indica que algunos pacientes tienen niveles extremadamente altos o bajos\n",
    "de ciertos biomarcadores.\n",
    "\n",
    "Valores faltantes: La presencia de valores faltantes en algunos biomarcadores debe ser considerada y abordada mediante técnicas de imputación o exclusión de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5lgiuTkzOYy"
   },
   "source": [
    "Distribución de Edad: La mayoría de los pacientes tienen edades que oscilan entre 40 y 70 años.\n",
    "\n",
    "Distribución de Sexo: Hay más mujeres que hombres en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H86fZ-YKzOYy"
   },
   "source": [
    "La gráfica muestra que el tipo de tumor más común en el conjunto de datos es el de colorrecto, seguido por otros tipos como el de pulmón y el de mama.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzKzotzNzOYz"
   },
   "source": [
    "Distribución del Volumen de Plasma: La mayoría de los volúmenes de plasma están entre 4 y 7 mL.\n",
    "\n",
    "Distribución de la Concentración de ADN en Plasma: La concentración de ADN en plasma varía ampliamente, pero la mayoría de las muestras tienen concentraciones más bajas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t854N2mzOYz"
   },
   "source": [
    "El ADN libre en sangre, bajo condiciones normales, se encuentra en muy bajas concentraciones, asociado a los procesos de muerte celular, que ocurre normalmente en el recambio celular, que a su vez se asocia entre otros al catabolismo aumentado de los glóbulos blancos en voluntarios sanos (1), lográndose cuantificar en el torrente sanguíneo en muestras como suero o plasma, además de muestras como orina, leche materna y saliva (2-4). El ADN libre es resistente a las RNAsas, debido a que éstas solo pueden degradar ADN de cadena sencilla (ssADN) mientras que el ADN que se encuentra libre en el torrente sanguíneo, generalmente es de doble cadena (5,6).\n",
    "Las concentraciones de ADN libre en sangre pueden verse alteradas por distintas circunstancias, que en general se pueden resumir en daño a tejidos, inflamación, embarazo, infecciones, cáncer y trauma (7-9). En pacientes enfermos con cáncer la concentración de ADN libre disminuye en respuesta a quimioterapia (10) y se incrementa cuando hay una diseminación del tumor o metástasis (10-13).\n",
    "\n",
    "Existe gran divergencia entre valores de referencia de la concentración de ADN libre en voluntarios sanos. Para Wu y colaboradores la concentración de ADN libre en individuos sanos es de 57,1 +/- 30,6 ng/mL (14); otros muestran concentraciones de 0 hasta 35,2 ug/mL (9); para otros autores, los valores normales son de 10 a 30ng/mL con una media de 13 ng/mL (1) o de 18 ng/mL (15). Adicionalmente, la concentración de este ADN aumenta en pacientes con cáncer excediendo los 100 ng/mL hasta aproximadamente 180 ng/mL (1,16).\n",
    "\n",
    "En pacientes con cáncer de pulmón se han reportado medias de 318 ng/mL (15). Se hace necesario establecer los valores normales de la cuantificación de ADN libre en diferentes regiones de población colombiana, y así poder establecer una comparación con pacientes que padezcan alguna de las patologías anteriormente mencionadas, pues la divergencia en estos valores reportados podría llegar a ser tal que, al realizar dichas comparaciones, se incurra en errores obteniéndose valores de cuantificación equívocos que puedan resultar en un informe deficiente a la hora de hacer determinaciones de enfermedades por medio de esta técnica.\n",
    "\n",
    "Este estudio tuvo como objetivo determinar la concentración de ADN libre en personas sanas en la población bogotana, para establecer un rango normal o rango de referencia mediante la técnica de PCR en Tiempo-Real sin realizar pasos de extracción y purificación de ADN.\n",
    "\n",
    "https://revistas.unicolmayor.edu.co/index.php/nova/article/view/139/279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xazm3tUwzOYz"
   },
   "source": [
    "## Cargar las tablas que vamos a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "error",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Y1P6EIPzzOYz",
    "outputId": "6f3329da-5b3c-4728-b7d4-2e3613fa999c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/34622/Desktop/Master_location/SEGUNDA_PARTE_TFM_porfa/Tables_S1_to_S11.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-641416054fd8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgroup_column_s6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Tumor type'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_s4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name_s4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_s6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name_s6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_s6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df6.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2336a8f98121>\u001b[0m in \u001b[0;36mload_and_clean_data\u001b[0;34m(file_path, sheet_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Función para cargar y limpiar datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_to_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1497\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     ) as handle:\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/34622/Desktop/Master_location/SEGUNDA_PARTE_TFM_porfa/Tables_S1_to_S11.xlsx'"
     ]
    }
   ],
   "source": [
    "# Variables para la ejecución\n",
    "file_path = '/content/Tables_S1_to_S11.xlsx'\n",
    "sheet_name_s4 = 'Table S4'\n",
    "sheet_name_s6 = 'Table S6'\n",
    "group_column_s6 = 'Tumor type'\n",
    "\n",
    "df_s4 = load_and_clean_data(file_path, sheet_name_s4)\n",
    "df_s6 = load_and_clean_data(file_path, sheet_name_s6)\n",
    "df_s6.to_excel('df6.xlsx')\n",
    "df_s4.to_excel('df4.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEKPY4HEzOYz"
   },
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "zNs4s1Y_zOYz"
   },
   "outputs": [],
   "source": [
    "df_s6_sin_normal = df_s6[df_s6['Tumor type'] != 'Normal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "W-CJraLpzOY0"
   },
   "outputs": [],
   "source": [
    "df_s6_sin_normal.to_excel('df6_sin_normal.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "IO7P9OlezOY0"
   },
   "outputs": [],
   "source": [
    "df_s6_sin_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "72gv4JRDzOY0"
   },
   "outputs": [],
   "source": [
    "df_s6_sin_normal.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP_WXnPTzOY0"
   },
   "source": [
    "* Omega score (0.70% Missing):\n",
    "\n",
    "    Recomendación: Dado el pequeño porcentaje de valores faltantes, la imputación con la mediana es adecuada para mantener la robustez ante posibles valores atípicos.\n",
    "\n",
    "* G-CSF (pg/ml) (0.10% Missing):\n",
    "\n",
    "    Recomendación: La imputación con la mediana también es adecuada aquí debido al muy bajo porcentaje de valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "hVsBr-wBzOY1"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Eliminar las columnas no deseadas\n",
    "columns_to_remove = ['Patient ID #', 'Sample ID #', 'CancerSEEK Test Result', 'CancerSEEK Logistic Regression Score']\n",
    "df_new = df_s6_sin_normal.drop(columns=columns_to_remove)\n",
    "\n",
    "# Guardar nuevo dataset FINAL\n",
    "df_new.to_excel('df_final.xlsx')\n",
    "\n",
    "# Identificar características numéricas y categóricas\n",
    "numeric_features = df_new.select_dtypes(include=[float, int]).columns.tolist()\n",
    "categorical_features = df_new.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Preprocesamiento para las características numéricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputación con la mediana\n",
    "    ('scaler', StandardScaler())  # Estandarización\n",
    "])\n",
    "\n",
    "# Preprocesamiento para las características categóricas ordinales\n",
    "ordinal_features = ['AJCC Stage']\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(dtype=int))  # Codificación Ordinal\n",
    "])\n",
    "\n",
    "# Preprocesamiento para las características categóricas nominales (binarias)\n",
    "nominal_features = ['Sex']\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='if_binary', dtype=int))  # Codificación binaria\n",
    "])\n",
    "\n",
    "# Combinación de los transformadores en un preprocesador\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features),\n",
    "        ('nom', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "# Separar las características (X) y la variable objetivo (y)\n",
    "X = df_new.drop(columns=['Tumor type'])\n",
    "y = df_new['Tumor type']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Ajustar y transformar los datos de entrenamiento y prueba\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocesamiento completado, datos filtrados y divididos en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekfewhLwzOY1"
   },
   "source": [
    "**Pasos seguidos**\n",
    "\n",
    "Eliminar Columnas No Deseadas: Se eliminaron las siguientes columnas:\n",
    "\n",
    "* Patient ID #\n",
    "* Sample ID #\n",
    "* CancerSEEK Test Result\n",
    "* CancerSEEK Logistic Regression Score\n",
    "\n",
    "Identificación de Características:\n",
    "\n",
    "* Características Numéricas: Se identificaron las columnas numéricas restantes después de la eliminación.\n",
    "* Características Categóricas: Se identificaron las columnas categóricas restantes.\n",
    "\n",
    "Preprocesamiento de las Características Numéricas:\n",
    "\n",
    "* Imputación con la Mediana: Para manejar valores nulos en características numéricas.\n",
    "* Estandarización: Para escalar las características numéricas a una distribución estándar.\n",
    "\n",
    "Preprocesamiento de las Características Categóricas:\n",
    "\n",
    "* Ordinal Encoding para AJCC Stage: Codificación ordinal, ya que tiene un orden intrínseco.\n",
    "* Codificación Binaria para Sex: Codificación binaria, dado que es una variable con dos categorías (Male y Female).\n",
    "* Combinación de los Transformadores: Se combinó todo el preprocesamiento en un solo ColumnTransformer para aplicar las transformaciones adecuadas a cada tipo de característica.\n",
    "\n",
    "Separación de Características y Variable Objetivo:\n",
    "\n",
    "* Características (X): Se eliminaron las columnas seleccionadas para quedarse con las características.\n",
    "* Variable Objetivo (y): Tumor type.\n",
    "\n",
    "División en Conjuntos de Entrenamiento y Prueba:\n",
    "\n",
    "* Entrenamiento (70%) y Prueba (30%): Los datos se dividieron aleatoriamente en conjuntos de entrenamiento y prueba usando una semilla aleatoria (random_state=42) para reproducibilidad.\n",
    "\n",
    "Ajuste y Transformación de los Datos:\n",
    "\n",
    "* Se ajustaron los datos de entrenamiento con el preprocesador configurado.\n",
    "* Se transformaron los datos de entrenamiento y prueba aplicando las mismas transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1719477185903,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "xK2M4PyVzOY2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Función para combinar los datos transformados en un DataFrame\n",
    "def combine_transformed_data_full(X_train_transformed, X_test_transformed, preprocessor, y_train, y_test):\n",
    "    # Obtener nombres de las características numéricas y categóricas\n",
    "    num_features = preprocessor.transformers_[0][2]\n",
    "    ord_features = preprocessor.transformers_[1][2]\n",
    "    nom_features = preprocessor.transformers_[2][2]\n",
    "\n",
    "    # Obtener los nombres de las columnas después de la transformación\n",
    "    num_col_names = num_features\n",
    "    ord_col_names = ord_features\n",
    "    nom_col_names = preprocessor.transformers_[2][1]['onehot'].get_feature_names_out(nom_features)\n",
    "\n",
    "    # Combinar los nombres de las columnas\n",
    "    all_col_names = np.concatenate([num_col_names, ord_col_names, nom_col_names])\n",
    "\n",
    "    # Combinar los datos transformados de entrenamiento y prueba\n",
    "    X_combined = np.vstack([X_train_transformed, X_test_transformed])\n",
    "    y_combined = np.concatenate([y_train, y_test])\n",
    "\n",
    "    # Crear un DataFrame con los datos transformados\n",
    "    df_transformed = pd.DataFrame(X_combined, columns=all_col_names)\n",
    "\n",
    "    # Añadir la columna objetivo al DataFrame transformado\n",
    "    df_transformed['Tumor type'] = y_combined\n",
    "\n",
    "    # Convertir las columnas ordinales y nominales a enteros\n",
    "    df_transformed[ord_features] = df_transformed[ord_features].astype(int)\n",
    "    df_transformed[nom_col_names] = df_transformed[nom_col_names].astype(int)\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "# Transformar la variable objetivo\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Combinar los datos transformados de entrenamiento y prueba con la variable objetivo codificada\n",
    "df_combined_transformed = combine_transformed_data_full(X_train, X_test, preprocessor, y_train_encoded, y_test_encoded)\n",
    "\n",
    "# Guardar el DataFrame combinado y transformado en un archivo CSV\n",
    "df_combined_transformed.to_csv('transformed_combined_dataframe.csv', index=False)\n",
    "\n",
    "# Mostrar el DataFrame combinado y transformado\n",
    "df_combined_transformed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOwH55MzzOY2"
   },
   "source": [
    "Hemos transformado la variable objetivo a formato numerico por las siguientes razones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNFEeif8zOY2"
   },
   "source": [
    "* Compatibilidad: Asegura que la variable objetivo sea compatible con una amplia gama de algoritmos de machine learning.\n",
    "* Eficiencia: Los algoritmos de aprendizaje suelen funcionar más eficientemente con variables numéricas.\n",
    "* Precisión: Evita posibles errores de procesamiento que pueden ocurrir con variables categóricas en formato string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185904,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Rw8ttcyWzOY2"
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A0fSstSzOY3"
   },
   "source": [
    "La distribución muestra un claro desbalance, donde ciertas categorías (Colorectum, Breast) tienen muchos más ejemplos que otras (Esophagus, Liver). Este desbalance puede afectar el rendimiento de los modelos de machine learning, especialmente en términos de sesgo hacia las clases más frecuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqDihMf-zOZB"
   },
   "source": [
    "## Desbalanceo y evaluacion de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185904,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "uiRjJQNdzOZB"
   },
   "outputs": [],
   "source": [
    "pip install pandas scikit-learn openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185904,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "ZK2NJlgnzOZC"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from ctgan import CTGANSynthesizer\n",
    "from imblearn.under_sampling import RandomUnderSampler, CondensedNearestNeighbour, NearMiss\n",
    "from ctgan import CTGANSynthesizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Definir funciones para evaluación y resultados\n",
    "def mostrar_estadisticas_guardar_tabla(y_val, y_pred, set_name, model_name):\n",
    "    global tabla_results_df\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "    print(f'Model performance for {set_name} - {model_name}')\n",
    "    print(\"- Accuracy: {:.4f}\".format(accuracy))\n",
    "    print('- F1 score: {:.4f}'.format(f1))\n",
    "    print('- Precision: {:.4f}'.format(precision))\n",
    "    print('- Recall: {:.4f}'.format(recall))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "    global_score = calcular_puntuacion_global(accuracy, precision, recall, f1)\n",
    "\n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Set': [set_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1-Score': [f1],\n",
    "        'Global Score' : [global_score]\n",
    "    })\n",
    "\n",
    "    tabla_results_df = pd.concat([tabla_results_df, new_row], ignore_index=True)\n",
    "\n",
    "    return tabla_results_df\n",
    "\n",
    "def calcular_puntuacion_global(accuracy, precision, recall, f1):\n",
    "    # Definir ponderaciones para cada métrica\n",
    "    weights = {\n",
    "        'accuracy': 0.25,\n",
    "        'precision': 0.25,\n",
    "        'recall': 0.25,\n",
    "        'f1': 0.25,\n",
    "    }\n",
    "\n",
    "    # Calcular la puntuación global\n",
    "    global_score = (accuracy * weights['accuracy'] +\n",
    "                    precision * weights['precision'] +\n",
    "                    recall * weights['recall'] +\n",
    "                    f1 * weights['f1'])\n",
    "\n",
    "    return round(global_score * 100, 2)\n",
    "\n",
    "\n",
    "# Función para entrenar y evaluar modelos\n",
    "def entrenar_y_evaluar_modelo(X_train, y_train, X_test, y_test, model, model_name):\n",
    "    try:\n",
    "        # Cross validation\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=1)\n",
    "        print(f\"Cross-validated accuracy: {scores.mean():.4f}\")\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluar en el conjunto de entrenamiento\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name)\n",
    "\n",
    "        # Evaluar en el conjunto de prueba\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al entrenar o evaluar el modelo {model_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185904,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "f19afbae"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_synthetic_data_CTGAN(X_train, y_train):\n",
    "    # Combine X and y for CTGAN\n",
    "    data = X_train.copy()\n",
    "    data['target'] = y_train\n",
    "\n",
    "    # Initialize and train CTGAN\n",
    "    ctgan = CTGANSynthesizer()\n",
    "    ctgan.fit(data, ['target'])\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    synthetic_data = ctgan.sample(len(X_train))\n",
    "\n",
    "    # Separate X and y\n",
    "    X_synth = synthetic_data.drop('target', axis=1)\n",
    "    y_synth = synthetic_data['target']\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    X_train_combined = pd.concat([X_train, X_synth], ignore_index=True)\n",
    "    y_train_combined = pd.concat([y_train, y_synth], ignore_index=True)\n",
    "\n",
    "    return X_train_combined, y_train_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1719477185904,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "EJ1FzZZnzOZC"
   },
   "outputs": [],
   "source": [
    "# Inicializar el DataFrame de resultados\n",
    "tabla_results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Global Score'])\n",
    "\n",
    "# Definir los modelos y estrategias de balanceo\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "estrategias_balanceo = {\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42),\n",
    "    'NearMiss': NearMiss(),\n",
    "    'CondensedNearestNeighbour': CondensedNearestNeighbour(random_state=42),\n",
    "    'CTGAN': CTGAN(random_state=42),\n",
    "    'CTGANENN': CTGANENN(random_state=42),\n",
    "    'CTGAN + RandomUnderSampler': Pipeline([\n",
    "        ('ctgan', CTGAN(random_state=42)),\n",
    "        ('under', RandomUnderSampler(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = df_combined_transformed.drop(columns=['Tumor type'])\n",
    "y = df_combined_transformed['Tumor type']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Entrenar y evaluar modelos para cada estrategia de balanceo\n",
    "for estrategia_nombre, estrategia in estrategias_balanceo.items():\n",
    "    print(f\"\\nEstrategia de balanceo: {estrategia_nombre}\")\n",
    "    if estrategia_nombre == 'CTGAN + RandomUnderSampler':\n",
    "        X_train_balanced, y_train_balanced = estrategia.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_train_balanced, y_train_balanced = estrategia.fit_resample(X_train, y_train)\n",
    "\n",
    "    for modelo_nombre, modelo in modelos.items():\n",
    "        print(f\"\\nModelo: {modelo_nombre}\")\n",
    "        model_name = f\"{modelo_nombre} ({estrategia_nombre})\"\n",
    "        entrenar_y_evaluar_modelo(X_train_balanced, y_train_balanced, X_test, y_test, modelo, model_name)\n",
    "\n",
    "# Guardar los resultados en un archivo Excel\n",
    "tabla_results_df.to_excel('model_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1DTXtqXzOZD"
   },
   "source": [
    "1. Random UnderSampler\n",
    "- Propósito: Esta técnica reduce el número de ejemplos en la clase mayoritaria para igualar el número de ejemplos en la clase minoritaria.\n",
    "- Ventaja: Es sencillo y rápido, y puede ser efectivo cuando se dispone de un gran número de datos.\n",
    "- Desventaja: Puede eliminar ejemplos importantes de la clase mayoritaria, lo que podría resultar en la pérdida de información valiosa.\n",
    "2. NearMiss\n",
    "- Propósito: NearMiss es una técnica de submuestreo que selecciona ejemplos de la clase mayoritaria basados en su proximidad a la clase minoritaria.\n",
    "- Ventaja: Mantiene ejemplos representativos de la clase mayoritaria cerca de la frontera de decisión, lo que puede ayudar en la clasificación.\n",
    "- Desventaja: Como el Random UnderSampler, puede eliminar ejemplos importantes, y puede ser computacionalmente costoso.\n",
    "3. Condensed Nearest Neighbour (CNN)\n",
    "- Propósito: CNN elimina ejemplos redundantes de la clase mayoritaria y conserva un subconjunto que representa bien la frontera de decisión.\n",
    "- Ventaja: Reduce el tamaño del conjunto de datos sin perder ejemplos críticos para la clasificación.\n",
    "- Desventaja: Puede ser computacionalmente costoso y no siempre funciona bien con datos de alta dimensionalidad.\n",
    "4. CTGAN (Conditional Tabular GAN)\n",
    "- Propósito: CTGAN genera datos sintéticos similares a los datos reales aprendiendo la distribución conjunta de las características del conjunto de datos original.\n",
    "- Ventaja: Aumenta el número de ejemplos de la clase minoritaria de una manera más realista al capturar complejas dependencias entre características, lo que puede mejorar la capacidad del modelo para aprender características de la clase minoritaria.\n",
    "- Desventaja: Puede ser computacionalmente costoso y requerir una cantidad significativa de tiempo para entrenar, además de ser sensible a la calidad y cantidad de los datos de entrenamiento.\n",
    "5. CTGAN + Edited Nearest Neighbours (ENN)\n",
    "- Propósito: Combina CTGAN con Edited Nearest Neighbours (ENN), primero generando ejemplos sintéticos y luego eliminando ejemplos problemáticos tanto de la clase mayoritaria como de la minoritaria.\n",
    "- Ventaja: Mejora la calidad del conjunto de datos al combinar las ventajas de generación de datos sintéticos realistas y limpieza de datos.\n",
    "- Desventaja: Es más complejo y computacionalmente costoso que aplicar CTGAN o ENN por separado.\n",
    "6. CTGAN + RandomUnderSampler\n",
    "- Propósito: Aplica CTGAN para generar ejemplos sintéticos de la clase minoritaria y luego utiliza Random UnderSampler para reducir el número de ejemplos en la clase mayoritaria.\n",
    "- Ventaja: Combina las ventajas de generación de datos sintéticos realistas y submuestreo, proporcionando un equilibrio entre aumentar la clase minoritaria y reducir la clase mayoritaria.\n",
    "- Desventaja: Puede ser computacionalmente costoso y requiere un ajuste cuidadoso para evitar la eliminación de ejemplos importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nTQ9wJCzOZD"
   },
   "source": [
    "### Interpretacion de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "OhfKvx0TzOZD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo de resultados\n",
    "results_df = pd.read_excel('model_results.xlsx')\n",
    "\n",
    "# Ordenar por Global Score y mostrar los mejores modelos en el conjunto de prueba\n",
    "best_test_models = results_df[results_df['Set'] == 'Test'].sort_values(by='Global Score', ascending=False)\n",
    "\n",
    "# Mostrar los tres mejores modelos\n",
    "top_3_models = best_test_models.head(3)\n",
    "\n",
    "# Imprimir los detalles de los tres mejores modelos\n",
    "for i, model in top_3_models.iterrows():\n",
    "    print(f\"Modelo: {model['Model']} con estrategia de balanceo {model['Set']} con Global Score: {model['Global Score']:.2f}\")\n",
    "\n",
    "# También devolver el DataFrame de los tres mejores modelos para una inspección adicional si es necesario\n",
    "top_3_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzI36hhhzOZD"
   },
   "source": [
    "### Busqueda de variables que aportan mas informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106254,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "9uK9tqmqzOZD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Asegurarse de que no hay datos faltantes\n",
    "df = df_combined_transformed.dropna()\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df.drop(columns=['Tumor type'])  # Asumiendo que la columna de destino se llama 'CancerType'\n",
    "y = df['Tumor type']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo de Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=250, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importances = rf.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Crear un DataFrame con las importancias de las características\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Graficar la importancia de las características\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Importancia de las Características')\n",
    "plt.xlabel('Ganancia de Información (InfoG)')\n",
    "plt.ylabel('Características')\n",
    "plt.show()\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "importance_df.to_csv('resultados/feature_importances.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106253,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "bmDkQLtIzOZE"
   },
   "outputs": [],
   "source": [
    "# 20 primeros features\n",
    "top_20_importance_df = importance_df.head(20)\n",
    "top_20_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnzumVQBzOZE"
   },
   "source": [
    "### Evaluación del rendimiento de los modelos optimizados en un conjunto de prueba para asegurar que estas variables ofrecen gran parte de la informacion relevante en nuestro dataset\n",
    "Ventaja: Disminución en computación.\n",
    "\n",
    "Vamos a comprobar la disminución en tiempo de la ejecucion de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106251,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "ITirMwQ-zOZE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from ctgan import CTGANSynthesizer\n",
    "from ctgan import CTGANSynthesizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Función para calcular el Global Score\n",
    "def calcular_global_score(accuracy, precision, recall, f1):\n",
    "    weights = {\n",
    "        'accuracy': 0.25,\n",
    "        'precision': 0.25,\n",
    "        'recall': 0.25,\n",
    "        'f1': 0.25,\n",
    "    }\n",
    "\n",
    "    global_score = (accuracy * weights['accuracy'] +\n",
    "                    precision * weights['precision'] +\n",
    "                    recall * weights['recall'] +\n",
    "                    f1 * weights['f1'])\n",
    "\n",
    "    return round(global_score * 100, 2)\n",
    "\n",
    "# Función para mostrar estadísticas y guardar en una tabla\n",
    "def mostrar_estadisticas_guardar_tabla(y_true, y_pred, model_name, results_df):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    global_score = calcular_global_score(accuracy, precision, recall, f1)\n",
    "\n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Set': ['Test'],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1-Score': [f1],\n",
    "        'Global Score': [global_score]\n",
    "    })\n",
    "\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    return results_df\n",
    "\n",
    "# Resultados anteriores\n",
    "previous_best_results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Random Forest (CTGAN)',\n",
    "        'Gradient Boosting (CTGAN)',\n",
    "        'Gradient Boosting (CTGAN + RandomUnderSampler)'\n",
    "    ],\n",
    "    'Set': ['Test', 'Test', 'Test'],\n",
    "    'Accuracy': [0.741722, 0.721854, 0.718543],\n",
    "    'Precision': [0.740079, 0.720030, 0.716450],\n",
    "    'Recall': [0.741722, 0.721854, 0.718543],\n",
    "    'F1-Score': [0.738834, 0.718526, 0.716625],\n",
    "    'Global Score': [74.06, 72.06, 71.75]\n",
    "})\n",
    "\n",
    "# Cargar tus datos transformados\n",
    "# df_combined_transformed = pd.read_excel('df_combined_transformed.xlsx')\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = df_combined_transformed.drop(columns=['Tumor type'])\n",
    "y = df_combined_transformed['Tumor type']\n",
    "\n",
    "# Seleccionar las características más importantes (decrease in purity)\n",
    "important_features = [\n",
    "    'sFas (pg/ml)',\n",
    "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
    "    'CA 15-3 (U/ml)',\n",
    "    'CA19-9 (U/ml)',\n",
    "    'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)',\n",
    "    'TGFa (pg/ml)',\n",
    "    'Sex_Male',\n",
    "    'Leptin (pg/ml)',\n",
    "    'IL-8 (pg/ml)',\n",
    "    'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)',\n",
    "    'GDF15 (ng/ml)',\n",
    "    'Prolactin (pg/ml)',\n",
    "    'HGF (pg/ml)',\n",
    "    'CD44 (ng/ml)',\n",
    "    'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)',\n",
    "    'TIMP-1 (pg/ml)',\n",
    "    'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "\n",
    "# Filtrar el conjunto de datos con las características seleccionadas\n",
    "X_important = X[important_features]\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_important, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Inicializar el DataFrame de resultados\n",
    "results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Global Score'])\n",
    "\n",
    "# Modelos a evaluar con pipelines de balanceo\n",
    "pipelines = {\n",
    "    'Random Forest (Important Features + CTGAN)': Pipeline([\n",
    "        ('ctgan', CTGAN(random_state=42)),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Gradient Boosting (Important Features + CTGAN)': Pipeline([\n",
    "        ('ctgan', CTGAN(random_state=42)),\n",
    "        ('clf', GradientBoostingClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Random Forest (Important Features + CTGAN + RandomUnderSampler)': Pipeline([\n",
    "        ('ctganenn', CTGANENN(random_state=42)),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Gradient Boosting (Important Features + CTGAN + RandomUnderSampler)': Pipeline([\n",
    "        ('ctganenn', CTGANENN(random_state=42)),\n",
    "        ('clf', GradientBoostingClassifier(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"\\n{model_name} - Conjunto de Prueba\")\n",
    "    results_df = mostrar_estadisticas_guardar_tabla(y_test, y_pred_test, model_name + \" (Test)\", results_df)\n",
    "\n",
    "# Comparar con los mejores resultados anteriores\n",
    "comparison_df = pd.concat([previous_best_results, results_df], ignore_index=True)\n",
    "comparison_df.to_excel('comparison_with_important_features.xlsx', index=False)\n",
    "print(\"Comparación de resultados guardada en 'comparison_with_important_features.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzUZM-yCzOZF"
   },
   "source": [
    "Comparandolo con los resultados de los modelos podemos ver que estas 20 caracteristicas puede acercarse mucho al global score con todas las caracteristicas:\n",
    "\n",
    "\n",
    "### Modelos con Características Importantes:\n",
    "| Model                                         | Accuracy | Precision | Recall   | F1-Score | Global Score |\n",
    "|-----------------------------------------------|----------|-----------|----------|----------|--------------|\n",
    "| Random Forest (Important Features + SMOTE) (Test) | 0.728477 | 0.758627  | 0.728477 | 0.735870 | 73.79        |\n",
    "\n",
    "### Modelos sin Características Importantes:\n",
    "| Model                | Accuracy | Precision | Recall   | F1-Score | Global Score |\n",
    "|----------------------|----------|-----------|----------|----------|--------------|\n",
    "| Random Forest (SMOTE) | 0.741722 | 0.740079  | 0.741722 | 0.738834 | 74.06        |\n",
    "\n",
    "Podemos apreciar que el tiempo de ejecución ha disminuido notablemente. De 44 minutos con todas las variables a 12 minutos con mas o menos la mitad de las variables en nuestro dataset, dandonos unos resultados casi semejantes.\n",
    "\n",
    "**Conclusión**: La extración de variables más importantes es buena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106250,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "x3_d6TXwzOZF"
   },
   "outputs": [],
   "source": [
    "# Guardamos en excel para facilidad de llamado del dataset y registro (Buenas prácticas)\n",
    "df_combined_transformed.to_excel('df_transformado.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCJ0fXDvzOZF"
   },
   "source": [
    "### Evaluación de Rendimiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106248,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "g3ze9sN3zOZF"
   },
   "outputs": [],
   "source": [
    "# Cargar el archivo de resultados\n",
    "results_df = pd.read_excel('model_results_top_features.xlsx')\n",
    "\n",
    "# Ordenar por Global Score y mostrar los mejores modelos en el conjunto de prueba\n",
    "best_test_models = results_df[results_df['Set'] == 'Test'].sort_values(by='Global Score', ascending=False)\n",
    "\n",
    "# Mostrar los tres mejores modelos\n",
    "top_3_models = best_test_models.head(3)\n",
    "\n",
    "# Imprimir los detalles de los tres mejores modelos\n",
    "for i, model in top_3_models.iterrows():\n",
    "    print(f\"Modelo: {model['Model']} con estrategia de balanceo {model['Set']} con Global Score: {model['Global Score']:.2f}\")\n",
    "\n",
    "# También devolver el DataFrame de los tres mejores modelos para una inspección adicional si es necesario\n",
    "top_3_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkphLhUqzOZG"
   },
   "source": [
    "Gradien Boosting (SMOTE + RandomUnderSampler) Hypertunning     ## POR HACER (MUY PESADO COMPUTACIONALMENTE) NO se si merece la pena en cuanto a tiempo y mejora del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106246,
     "status": "aborted",
     "timestamp": 1719477185905,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "d3efQ6kuzOZG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from ctgan import CTGANSynthesizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Inicializar DataFrame para almacenar resultados\n",
    "tabla_results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Global Score'])\n",
    "\n",
    "# Asegurarse de que no hay datos faltantes\n",
    "df = df_combined_transformed.dropna()\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)',\n",
    "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
    "    'CA 15-3 (U/ml)',\n",
    "    'CA19-9 (U/ml)',\n",
    "    'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)',\n",
    "    'TGFa (pg/ml)',\n",
    "    'Sex_Male',\n",
    "    'Leptin (pg/ml)',\n",
    "    'IL-8 (pg/ml)',\n",
    "    'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)',\n",
    "    'GDF15 (ng/ml)',\n",
    "    'Prolactin (pg/ml)',\n",
    "    'HGF (pg/ml)',\n",
    "    'CD44 (ng/ml)',\n",
    "    'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)',\n",
    "    'TIMP-1 (pg/ml)',\n",
    "    'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df[important_features]\n",
    "y = df['Tumor type']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('ctgan', CTGAN(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Definir la búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': randint(100, 500),\n",
    "    'classifier__learning_rate': uniform(0.01, 0.3),\n",
    "    'classifier__max_depth': randint(3, 10),\n",
    "    'classifier__subsample': uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda de hiperparámetros\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=50, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener las mejores hiperparámetros\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Mejores hiperparámetros: {best_params}\")\n",
    "\n",
    "# Evaluar el modelo\n",
    "entrenar_y_evaluar_modelo(X_train, y_train, X_test, y_test, random_search.best_estimator_, \"Gradient Boosting (CTGAN + RandomUnderSampler)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnW4PcwDzOZG"
   },
   "source": [
    "# XGBoost y LightGBM en la Detección de Tipos de Cáncer con Variable Objetivo Desbalanceada\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) y LightGBM (Light Gradient Boosting Machine) son algoritmos de boosting que han demostrado ser extremadamente eficaces en una amplia gama de problemas de clasificación y regresión, incluidos aquellos con variables objetivo desbalanceadas, como la detección de tipos de cáncer. Aquí se explican las razones de su efectividad:\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "1. **Boosting por Gradiente**:\n",
    "   - XGBoost es un algoritmo de boosting por gradiente que crea árboles de decisión de manera secuencial, donde cada árbol intenta corregir los errores del árbol anterior. Esto permite una alta capacidad predictiva.\n",
    "\n",
    "2. **Manejo de Datos Desbalanceados**:\n",
    "   - XGBoost incluye parámetros específicos para manejar el desbalanceo de clases, como `scale_pos_weight`, que ajusta el peso de las clases para contrarrestar el desequilibrio. Este ajuste ayuda a que el modelo sea más sensible a la clase minoritaria, que en el caso de la detección de cáncer, puede ser crucial.\n",
    "\n",
    "3. **Regularización**:\n",
    "   - XGBoost implementa técnicas de regularización (L1 y L2), que ayudan a reducir el sobreajuste, mejorando así la capacidad del modelo para generalizar en datos nuevos.\n",
    "\n",
    "4. **Eficiencia Computacional**:\n",
    "   - Utiliza optimizaciones a nivel de hardware y software, como el uso de multiprocesamiento y la implementación de árboles de decisión altamente optimizados. Esto permite manejar grandes volúmenes de datos de manera eficiente.\n",
    "\n",
    "5. **Importancia de Características**:\n",
    "   - XGBoost proporciona medidas de importancia de las características, lo que permite identificar los biomarcadores más relevantes en el diagnóstico de tipos de cáncer.\n",
    "\n",
    "## LightGBM\n",
    "\n",
    "1. **Boosting por Hojas (Leaf-wise)**:\n",
    "   - LightGBM utiliza un método de crecimiento de árboles basado en hojas, en lugar de niveles (nivel por nivel). Esto le permite manejar datos desbalanceados de manera más efectiva, ya que puede concentrarse en las hojas con mayor error.\n",
    "\n",
    "2. **Tratamiento del Desbalanceo**:\n",
    "   - Similar a XGBoost, LightGBM tiene parámetros como `is_unbalance` y `scale_pos_weight`, que permiten al modelo ajustarse automáticamente para manejar datos desbalanceados.\n",
    "\n",
    "3. **Velocidad y Eficiencia**:\n",
    "   - LightGBM es conocido por su alta velocidad y eficiencia, especialmente en grandes conjuntos de datos. Utiliza técnicas como histogram-based decision tree learning para acelerar el proceso de entrenamiento.\n",
    "\n",
    "4. **Capacidad de Escalabilidad**:\n",
    "   - Puede manejar grandes volúmenes de datos con alta dimensionalidad, lo que es común en datasets médicos. Esto se debe a su diseño optimizado para memoria y capacidad de paralelización.\n",
    "\n",
    "5. **Reducción del Overfitting**:\n",
    "   - Incluye técnicas avanzadas de regularización y parámetros ajustables que permiten minimizar el riesgo de sobreajuste, mejorando la precisión en datos no vistos.\n",
    "\n",
    "## Aplicación en la Detección de Tipos de Cáncer\n",
    "\n",
    "- **Detección Temprana y Precisa**:\n",
    "  Ambos algoritmos permiten una detección temprana y precisa de tipos de cáncer al identificar patrones complejos y sutiles en los datos que pueden no ser detectables con otros métodos.\n",
    "\n",
    "- **Adaptabilidad a Datos Desbalanceados**:\n",
    "  En los datos médicos, las clases desbalanceadas son comunes (por ejemplo, más casos de no-cáncer que de cáncer). La capacidad de estos algoritmos para ajustar los pesos de las clases permite mejorar la sensibilidad y especificidad de la detección.\n",
    "\n",
    "- **Identificación de Biomarcadores**:\n",
    "  La importancia de las características derivadas de estos modelos ayuda a identificar los biomarcadores clave para el diagnóstico y pronóstico del cáncer.\n",
    "\n",
    "En resumen, XGBoost y LightGBM son algoritmos potentes y eficientes que manejan bien los datos desbalanceados y ofrecen una gran capacidad predictiva y eficiencia computacional, lo que los hace ideales para aplicaciones críticas como la detección de tipos de cáncer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106246,
     "status": "aborted",
     "timestamp": 1719477185906,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Q7OozPqtzOZG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from ctgan import CTGANSynthesizer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Asegurarse de que no hay datos faltantes\n",
    "df = df_combined_transformed.dropna()\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)',\n",
    "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
    "    'CA 15-3 (U/ml)',\n",
    "    'CA19-9 (U/ml)',\n",
    "    'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)',\n",
    "    'TGFa (pg/ml)',\n",
    "    'Sex_Male',\n",
    "    'Leptin (pg/ml)',\n",
    "    'IL-8 (pg/ml)',\n",
    "    'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)',\n",
    "    'GDF15 (ng/ml)',\n",
    "    'Prolactin (pg/ml)',\n",
    "    'HGF (pg/ml)',\n",
    "    'CD44 (ng/ml)',\n",
    "    'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)',\n",
    "    'TIMP-1 (pg/ml)',\n",
    "    'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df[important_features]\n",
    "y = df['Tumor type']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Aplicar CTGAN al conjunto de entrenamiento\n",
    "ctgan = CTGAN(random_state=42)\n",
    "X_train_balanced, y_train_balanced = ctgan.fit_resample(X_train, y_train)\n",
    "\n",
    "# Definir los pipelines y modelos\n",
    "models = {\n",
    "    \"XGBoost (CTGAN)\": xgb.XGBClassifier(n_estimators=150, max_depth=7, learning_rate=0.1, random_state=42),\n",
    "    \"LightGBM (CTGAN)\": lgb.LGBMClassifier(n_estimators=150, max_depth=7, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar modelos\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('ctgan', CTGAN(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    entrenar_y_evaluar_modelo(X_train_balanced, y_train_balanced, X_test, y_test, pipeline, model_name)\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "tabla_results_df.to_csv('XGBoost_LightGBM.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106244,
     "status": "aborted",
     "timestamp": 1719477185906,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "F2bz3-dhzOZH"
   },
   "outputs": [],
   "source": [
    "XGBoost_LightGBM = pd.read_csv('XGBoost_LightGBM.csv')\n",
    "XGBoost_LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olagHNwczOZH"
   },
   "source": [
    "Hemos mejorado el modelo con un Score global de 76.30 lo que supone que vamos por buen camino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl0afT_fzOZH"
   },
   "source": [
    "LightGBM Hypertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106243,
     "status": "aborted",
     "timestamp": 1719477185906,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "JimD9eMfzOZH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from ctgan import CTGANSynthesizer\n",
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Inicializar DataFrame para almacenar resultados\n",
    "tabla_results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Global Score'])\n",
    "\n",
    "# Asegurarse de que no hay datos faltantes\n",
    "df = df_combined_transformed.dropna()\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)',\n",
    "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
    "    'CA 15-3 (U/ml)',\n",
    "    'CA19-9 (U/ml)',\n",
    "    'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)',\n",
    "    'TGFa (pg/ml)',\n",
    "    'Sex_Male',\n",
    "    'Leptin (pg/ml)',\n",
    "    'IL-8 (pg/ml)',\n",
    "    'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)',\n",
    "    'GDF15 (ng/ml)',\n",
    "    'Prolactin (pg/ml)',\n",
    "    'HGF (pg/ml)',\n",
    "    'CD44 (ng/ml)',\n",
    "    'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)',\n",
    "    'TIMP-1 (pg/ml)',\n",
    "    'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df[important_features]\n",
    "y = df['Tumor type']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Aplicar CTGAN al conjunto de entrenamiento\n",
    "ctgan = CTGAN(random_state=42)\n",
    "X_train_balanced, y_train_balanced = ctgan.fit_resample(X_train, y_train)\n",
    "\n",
    "# Definir los parámetros de búsqueda para LightGBM\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': randint(100, 500),\n",
    "    'classifier__max_depth': randint(3, 15),\n",
    "    'classifier__learning_rate': uniform(0.01, 0.3),\n",
    "    'classifier__num_leaves': randint(20, 50),\n",
    "    'classifier__min_child_samples': randint(10, 50),\n",
    "    'classifier__subsample': uniform(0.6, 0.9),\n",
    "    'classifier__colsample_bytree': uniform(0.6, 0.9)\n",
    "}\n",
    "\n",
    "# Definir el pipeline para LightGBM\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('ctgan', CTGAN(random_state=42)),\n",
    "    ('classifier', lgb.LGBMClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Configurar la búsqueda de hiperparámetros\n",
    "random_search = RandomizedSearchCV(lgb_pipeline, param_distributions=param_dist, n_iter=100, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Entrenar el modelo LightGBM con búsqueda de hiperparámetros\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Obtener las mejores hiperparámetros\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Mejores hiperparámetros: {best_params}\")\n",
    "\n",
    "# Evaluar el modelo mejorado\n",
    "entrenar_y_evaluar_modelo(X_train_balanced, y_train_balanced, X_test, y_test, random_search.best_estimator_, \"LightGBM (Optimizado)\")\n",
    "\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "tabla_results_df.to_csv('LightGBM_tunned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106241,
     "status": "aborted",
     "timestamp": 1719477185906,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Brc92ilwzOZI"
   },
   "outputs": [],
   "source": [
    "LightGBM_tunned = pd.read_csv('LightGBM_tunned.csv')\n",
    "LightGBM_tunned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLVGurJXzOZI"
   },
   "source": [
    "La empeoración del conjunto de datos puede deverse a:\n",
    "\n",
    "Variabilidad del Conjunto de Datos: El conjunto de datos puede tener  una variabilidad intrínseca que limita la capacidad de mejora a través del hiperajuste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Lp_yuDizOZI"
   },
   "source": [
    "# Ensamble de modelos (Voting Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wHYsb7IzOZI"
   },
   "source": [
    "## Utilización de un Ensamble de Modelos para Mejorar la Detección de Tipos de Cáncer\n",
    "\n",
    "La utilización de un ensamble de modelos es una técnica poderosa para mejorar el rendimiento y la robustez de los modelos de machine learning. En el contexto de detección de tipos de cáncer con variables objetivo desbalanceadas, el ensamble puede proporcionar varias ventajas significativas:\n",
    "\n",
    "## Razones para Utilizar un Ensamble de Modelos\n",
    "\n",
    "### Reducción del Overfitting\n",
    "\n",
    "- **Promedio de Errores**: Al combinar varios modelos, los errores específicos de cada modelo tienden a cancelarse entre sí. Esto ayuda a reducir el overfitting, ya que los modelos individuales pueden sobreajustarse a ruidos o patrones específicos del conjunto de entrenamiento, pero estos errores se compensan cuando se utilizan múltiples modelos.\n",
    "\n",
    "### Mejora de la Generalización\n",
    "\n",
    "- **Diversidad de Modelos**: Diferentes modelos pueden capturar diferentes aspectos de los datos. Por ejemplo, XGBoost y LightGBM, aunque ambos son métodos de boosting, tienen diferentes mecanismos internos que pueden captar distintas características del conjunto de datos. Un ensamble puede aprovechar estas diferencias y mejorar la capacidad de generalización del modelo final.\n",
    "\n",
    "### Estabilidad y Robustez\n",
    "\n",
    "- **Promedio de Resultados**: La combinación de múltiples modelos tiende a producir resultados más estables y robustos frente a variaciones en los datos. Esto es especialmente importante en aplicaciones críticas como la detección de cáncer, donde las predicciones erróneas pueden tener consecuencias graves.\n",
    "\n",
    "### Manejo de Datos Desbalanceados\n",
    "\n",
    "- **Balance de Clases**: Al utilizar técnicas de ensamble, se pueden diseñar estrategias específicas para manejar datos desbalanceados, como ajustar los pesos de las clases o utilizar técnicas de resampling dentro del ensamble. Esto puede mejorar la sensibilidad y especificidad de las predicciones para la clase minoritaria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPWoYc08zOZJ"
   },
   "source": [
    "# ¿Por que hemos escogido los siguientes modelos?\n",
    "\n",
    "## Razones para Usar Estos Modelos\n",
    "\n",
    "- **Mejores Métricas**: Estas combinaciones han demostrado ofrecer las mejores métricas de rendimiento en nuestras pruebas, lo que indica que son capaces de manejar bien los datos desbalanceados y proporcionar predicciones precisas.\n",
    "- **Diversidad en Modelos de Boosting**: Cada uno de estos algoritmos tiene mecanismos internos ligeramente diferentes y fortalezas únicas que, cuando se combinan, pueden mejorar la capacidad de generalización del modelo final.\n",
    "- **Reducción del Overfitting**: Al combinar varios modelos, se pueden cancelar los errores específicos de cada uno, reduciendo el riesgo de sobreajuste y mejorando la robustez del modelo.\n",
    "- **Robustez y Estabilidad**: La combinación de múltiples modelos tiende a producir resultados más estables y robustos frente a variaciones en los datos, lo cual es crucial en aplicaciones críticas como la detección de cáncer.\n",
    "\n",
    "En resumen, la elección de estas combinaciones de modelos de ensamble está respaldada por su rendimiento superior en términos de métricas y su capacidad para manejar datos desbalanceados, lo que los hace ideales para la tarea de detección de tipos de cáncer en nuestro caso específico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106240,
     "status": "aborted",
     "timestamp": 1719477185906,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "w0AbqyxqzOZJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from ctgan import CTGANSynthesizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Inicializar DataFrame para almacenar resultados\n",
    "tabla_results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Global Score'])\n",
    "\n",
    "# Asegurarse de que no hay datos faltantes\n",
    "df = df_combined_transformed.dropna()\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)',\n",
    "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
    "    'CA 15-3 (U/ml)',\n",
    "    'CA19-9 (U/ml)',\n",
    "    'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)',\n",
    "    'TGFa (pg/ml)',\n",
    "    'Sex_Male',\n",
    "    'Leptin (pg/ml)',\n",
    "    'IL-8 (pg/ml)',\n",
    "    'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)',\n",
    "    'GDF15 (ng/ml)',\n",
    "    'Prolactin (pg/ml)',\n",
    "    'HGF (pg/ml)',\n",
    "    'CD44 (ng/ml)',\n",
    "    'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)',\n",
    "    'TIMP-1 (pg/ml)',\n",
    "    'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df[important_features]\n",
    "y = df['Tumor type']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir los modelos base\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "lgbm = lgb.LGBMClassifier(random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Definir combinaciones de modelos para VotingClassifier\n",
    "combinations = [\n",
    "    ('Gradient Boosting + LightGBM', [('gb', gb), ('lgbm', lgbm)]),\n",
    "    ('Gradient Boosting + XGBoost', [('gb', gb), ('xgb', xgb_clf)]),\n",
    "    ('LightGBM + XGBoost', [('lgbm', lgbm), ('xgb', xgb_clf)]),\n",
    "    ('Gradient Boosting + LightGBM + XGBoost', [('gb', gb), ('lgbm', lgbm), ('xgb', xgb_clf)])\n",
    "]\n",
    "\n",
    "# Balancear el conjunto de entrenamiento con las técnicas especificadas\n",
    "# CTGAN + RandomUnderSampler para Gradient Boosting\n",
    "ctgan = CTGAN(random_state=42)\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_ctgan_rus, y_train_ctgan_rus = ctgan.fit_resample(X_train, y_train)\n",
    "X_train_ctgan_rus, y_train_ctgan_rus = under_sampler.fit_resample(X_train_ctgan_rus, y_train_ctgan_rus)\n",
    "\n",
    "# CTGAN para LightGBM y XGBoost\n",
    "X_train_ctgan, y_train_ctgan = ctgan.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entrenar y evaluar cada combinación\n",
    "for name, combination in combinations:\n",
    "    if 'Gradient Boosting' in name:\n",
    "        # Usar el conjunto balanceado con CTGAN + RandomUnderSampler\n",
    "        X_train_balanced = X_train_ctgan_rus\n",
    "        y_train_balanced = y_train_ctgan_rus\n",
    "    else:\n",
    "        # Usar el conjunto balanceado con CTGAN\n",
    "        X_train_balanced = X_train_ctgan\n",
    "        y_train_balanced = y_train_ctgan\n",
    "\n",
    "    voting_clf = VotingClassifier(estimators=combination, voting='soft')\n",
    "    entrenar_y_evaluar_modelo(X_train_balanced, y_train_balanced, X_test, y_test, voting_clf, name)\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "tabla_results_df.to_csv('Voting_clasiffier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106238,
     "status": "aborted",
     "timestamp": 1719477185906,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "9XFyIGHxzOZJ"
   },
   "outputs": [],
   "source": [
    "Voting_clasiffier = pd.read_csv('Voting_clasiffier.csv')\n",
    "Voting_clasiffier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi3gfkAxzOZJ"
   },
   "source": [
    "### Gradient Boosting + LightGBM + XGBoost\n",
    "\n",
    "Esta combinación tiene el mejor rendimiento en el conjunto de prueba con una precisión del 78.10% y una F1-Score de 78.35%. Esto sugiere que la inclusión de los tres modelos de boosting logra capturar mejor los patrones subyacentes en los datos, proporcionando la mejor capacidad de generalización entre todas las combinaciones probadas.\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "### Sobreajuste\n",
    "\n",
    "Todas las combinaciones muestran un alto grado de sobreajuste en el conjunto de entrenamiento. Es crucial abordar este problema para mejorar la capacidad de generalización de los modelos. Técnicas de regularización, validación cruzada y ajuste de hiperparámetros podrían ser útiles.\n",
    "\n",
    "### Mejora Gradual\n",
    "\n",
    "Vemos una mejora gradual en las métricas de rendimiento en el conjunto de prueba al combinar modelos adicionales. Esto confirma la hipótesis de que un ensamble de modelos puede capturar mejor los diversos patrones en los datos, compensando las debilidades de los modelos individuales.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCKgv_2SzOZJ"
   },
   "source": [
    "### El hypertunning es demasiado costoso computacionalmente, puede no merecer la pena. Seguimos explorando otro tipo de posibilidades..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1719477186318,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "4otIqaCqzOZK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from ctgan import CTGANSynthesizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Función para entrenar y evaluar modelos\n",
    "def entrenar_y_evaluar_modelo(X_train, y_train, X_test, y_test, modelo, name):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred_train = modelo.predict(X_train)\n",
    "    y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "    resultados_train = {\n",
    "        'Model': name,\n",
    "        'Set': 'Training',\n",
    "        'Accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'Precision': precision_score(y_train, y_pred_train, average='weighted'),\n",
    "        'Recall': recall_score(y_train, y_pred_train, average='weighted'),\n",
    "        'F1-Score': f1_score(y_train, y_pred_train, average='weighted'),\n",
    "        'Global Score': accuracy_score(y_train, y_pred_train) * 100\n",
    "    }\n",
    "\n",
    "    resultados_test = {\n",
    "        'Model': name,\n",
    "        'Set': 'Test',\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'Precision': precision_score(y_test, y_pred_test, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred_test, average='weighted'),\n",
    "        'F1-Score': f1_score(y_test, y_pred_test, average='weighted'),\n",
    "        'Global Score': accuracy_score(y_test, y_pred_test) * 100\n",
    "    }\n",
    "\n",
    "    return resultados_train, resultados_test\n",
    "\n",
    "# Inicializar DataFrame para almacenar resultados\n",
    "tabla_results_df = pd.DataFrame(columns=['Model', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Global Score'])\n",
    "\n",
    "# Asegurarse de que no hay datos faltantes\n",
    "df = df_combined_transformed.dropna()\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)', 'sHER2/sEGFR2/sErbB2 (pg/ml)', 'CA 15-3 (U/ml)', 'CA19-9 (U/ml)', 'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)', 'TGFa (pg/ml)', 'Sex_Male', 'Leptin (pg/ml)', 'IL-8 (pg/ml)', 'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)', 'GDF15 (ng/ml)', 'Prolactin (pg/ml)', 'HGF (pg/ml)', 'CD44 (ng/ml)', 'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)', 'TIMP-1 (pg/ml)', 'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df[important_features]\n",
    "y = df['Tumor type']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir los modelos base\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "lgbm = lgb.LGBMClassifier(random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Definir los parámetros para Grid Search\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [31, 41, 51]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Realizar Grid Search para Gradient Boosting\n",
    "grid_search_gb = GridSearchCV(estimator=gb, param_grid=param_grid_gb, cv=5, scoring='accuracy')\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "# Realizar Grid Search para LightGBM\n",
    "grid_search_lgbm = GridSearchCV(estimator=lgbm, param_grid=param_grid_lgbm, cv=5, scoring='accuracy')\n",
    "grid_search_lgbm.fit(X_train, y_train)\n",
    "best_lgbm = grid_search_lgbm.best_estimator_\n",
    "\n",
    "# Realizar Grid Search para XGBoost\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_clf, param_grid=param_grid_xgb, cv=5, scoring='accuracy')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Definir combinaciones de modelos para VotingClassifier\n",
    "combinations = [\n",
    "    ('Gradient Boosting + LightGBM', [('gb', best_gb), ('lgbm', best_lgbm)]),\n",
    "    ('Gradient Boosting + XGBoost', [('gb', best_gb), ('xgb', best_xgb)]),\n",
    "    ('LightGBM + XGBoost', [('lgbm', best_lgbm), ('xgb', best_xgb)]),\n",
    "    ('Gradient Boosting + LightGBM + XGBoost', [('gb', best_gb), ('lgbm', best_lgbm), ('xgb', best_xgb)])\n",
    "]\n",
    "\n",
    "# Balancear el conjunto de entrenamiento con las técnicas especificadas\n",
    "# CTGAN + RandomUnderSampler para Gradient Boosting\n",
    "ctgan = CTGAN(random_state=42)\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_ctgan_rus, y_train_ctgan_rus = ctgan.fit_resample(X_train, y_train)\n",
    "X_train_ctgan_rus, y_train_ctgan_rus = under_sampler.fit_resample(X_train_ctgan_rus, y_train_ctgan_rus)\n",
    "\n",
    "# CTGAN para LightGBM y XGBoost\n",
    "X_train_ctgan, y_train_ctgan = ctgan.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entrenar y evaluar cada combinación\n",
    "for name, combination in combinations:\n",
    "    if 'Gradient Boosting' in name:\n",
    "        # Usar el conjunto balanceado con CTGAN + RandomUnderSampler\n",
    "        X_train_balanced = X_train_ctgan_rus\n",
    "        y_train_balanced = y_train_ctgan_rus\n",
    "    else:\n",
    "        # Usar el conjunto balanceado con CTGAN\n",
    "        X_train_balanced = X_train_ctgan\n",
    "        y_train_balanced = y_train_ctgan\n",
    "\n",
    "    voting_clf = VotingClassifier(estimators=combination, voting='soft')\n",
    "    resultados_train, resultados_test = entrenar_y_evaluar_modelo(X_train_balanced, y_train_balanced, X_test, y_test, voting_clf, name)\n",
    "\n",
    "    # Agregar resultados al DataFrame\n",
    "    tabla_results_df = tabla_results_df.append(resultados_train, ignore_index=True)\n",
    "    tabla_results_df = tabla_results_df.append(resultados_test, ignore_index=True)\n",
    "\n",
    "# Guardar resultados en un archivo CSV\n",
    "tabla_results_df.to_csv('Voting_classifier_with_tuning.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck5YEXiXzOZK"
   },
   "source": [
    "### Si tenemos tiempo, correr el codigo (Puede tardar muchas horas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il7BaSOIzOZK"
   },
   "source": [
    "# No supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1719477186318,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "tcPW6zkCzOZK"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, Birch, MeanShift, OPTICS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, CondensedNearestNeighbour\n",
    "from ctgan import CTGANSynthesizer\n",
    "from ctgan import CTGANSynthesizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score, homogeneity_score, completeness_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Inicializar DataFrame para almacenar resultados\n",
    "tabla_results_df = pd.DataFrame(columns=['Model', 'Metric', 'Score'])\n",
    "\n",
    "def evaluar_clustering(X, y_true, labels, model_name):\n",
    "    if len(set(labels)) <= 1:\n",
    "        print(f\"Model {model_name} produjo un solo cluster. Ignorando métricas.\")\n",
    "        return\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    homogeneity = homogeneity_score(y_true, labels)\n",
    "    completeness = completeness_score(y_true, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X, labels)\n",
    "\n",
    "    print(f\"Resultados para {model_name}:\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "    print(f\"Homogeneity: {homogeneity:.4f}\")\n",
    "    print(f\"Completeness: {completeness:.4f}\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "\n",
    "    resultados = {\n",
    "        'Model': [model_name] * 4,\n",
    "        'Metric': ['Silhouette Score', 'Homogeneity', 'Completeness', 'Davies-Bouldin Index'],\n",
    "        'Score': [silhouette_avg, homogeneity, completeness, davies_bouldin]\n",
    "    }\n",
    "    global tabla_results_df\n",
    "    tabla_results_df = pd.concat([tabla_results_df, pd.DataFrame(resultados)], ignore_index=True)\n",
    "\n",
    "# Definir los modelos\n",
    "modelos = {\n",
    "    'KMeans': KMeans(n_clusters=7, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.3, min_samples=5),\n",
    "    'Agglomerative Clustering': AgglomerativeClustering(n_clusters=7),\n",
    "    'Birch': Birch(n_clusters=7),\n",
    "    'MeanShift': MeanShift(bandwidth=2),\n",
    "    'OPTICS': OPTICS(min_samples=5)\n",
    "}\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)',\n",
    "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
    "    'CA 15-3 (U/ml)',\n",
    "    'CA19-9 (U/ml)',\n",
    "    'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)',\n",
    "    'TGFa (pg/ml)',\n",
    "    'Sex_Male',\n",
    "    'Leptin (pg/ml)',\n",
    "    'IL-8 (pg/ml)',\n",
    "    'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)',\n",
    "    'GDF15 (ng/ml)',\n",
    "    'Prolactin (pg/ml)',\n",
    "    'HGF (pg/ml)',\n",
    "    'CD44 (ng/ml)',\n",
    "    'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)',\n",
    "    'TIMP-1 (pg/ml)',\n",
    "    'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiqueta\n",
    "X = df_combined_transformed[important_features]\n",
    "y = df_combined_transformed['Tumor type']\n",
    "\n",
    "# Reducir dimensionalidad con PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Entrenar y evaluar modelos\n",
    "for modelo_nombre, modelo in modelos.items():\n",
    "    print(f\"\\nModelo: {modelo_nombre}\")\n",
    "    model_name = f\"{modelo_nombre}\"\n",
    "    modelo.fit(X_pca)\n",
    "    labels = modelo.labels_ if hasattr(modelo, 'labels_') else modelo.predict(X_pca)\n",
    "    evaluar_clustering(X_pca, y, labels, model_name)\n",
    "\n",
    "# Guardar los resultados en un archivo Excel\n",
    "tabla_results_df.to_excel('Results_No_supervisado.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1719477186320,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "ZeWkngBgzOZL"
   },
   "outputs": [],
   "source": [
    "Results_No_supervisado = pd.read_excel('Results_No_supervisado.xlsx')\n",
    "Results_No_supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeItEB1OzOZL"
   },
   "source": [
    "# Conclusión sobre el Clustering No Supervisado en Nuestro Dataset\n",
    "\n",
    "Después de analizar los resultados de los distintos algoritmos de clustering no supervisado, podemos llegar a las siguientes conclusiones:\n",
    "\n",
    "## Análisis de Resultados\n",
    "\n",
    "### Silhouette Score\n",
    "\n",
    "- **Birch** y **MeanShift** obtuvieron los mejores puntajes, indicando clusters relativamente bien definidos.\n",
    "- Sin embargo, el resto de los modelos, especialmente **OPTICS** y **DBSCAN**, tuvieron puntajes bajos o negativos, sugiriendo una formación de clusters débil o inexistente.\n",
    "\n",
    "### Homogeneity y Completeness\n",
    "\n",
    "- Ninguno de los modelos mostró una homogeneidad o completitud significativamente alta, lo que indica que los clusters formados no corresponden claramente a ninguna estructura subyacente en las clases del dataset.\n",
    "\n",
    "### Davies-Bouldin Index\n",
    "\n",
    "- **Birch** y **MeanShift** obtuvieron los mejores índices, lo que respalda la idea de que forman clusters más compactos y separados.\n",
    "- No obstante, los otros modelos mostraron índices altos, sugiriendo clusters de baja calidad.\n",
    "\n",
    "## Interpretación General\n",
    "\n",
    "- **Clusters No Bien Definidos**: A pesar de que Birch y MeanShift han mostrado algunos resultados positivos, la calidad de los clusters generados por la mayoría de los algoritmos no es suficientemente buena para aportar valor significativo a nuestro problema.\n",
    "- **Heterogeneidad de Clusters**: La homogeneidad y completitud bajas sugieren que los clusters contienen una mezcla de clases, lo que indica que los modelos no están capturando bien las estructuras subyacentes en los datos.\n",
    "\n",
    "## Conclusión Final\n",
    "\n",
    "El análisis sugiere que el clustering no supervisado no aporta mucho valor a nuestro problema con este dataset. A pesar de que algunos algoritmos como Birch y MeanShift mostraron resultados decentes en ciertas métricas, la falta de clusters bien definidos y la baja homogeneidad y completitud indican que los modelos no están capturando estructuras significativas en los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZpOZixhzOZL"
   },
   "source": [
    "# Plan para Mejorar el Dataset Utilizando UMAP y KMeans\n",
    "\n",
    "Dado que los métodos de clustering no supervisado no han proporcionado métricas satisfactorias, hemos decidido adoptar una nueva estrategia para mejorar nuestro dataset. Nuestro objetivo principal es aumentar y enriquecer nuestro dataset, que actualmente cuenta con solo 1005 filas. Para ello, utilizaremos UMAP para la reducción de dimensionalidad y KMeans para identificar subgrupos, que luego añadiremos a nuestro dataset.\n",
    "\n",
    "## Objetivos y Pasos\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Mejorar el dataset actual de 1005 filas añadiendo información sobre subgrupos identificados mediante clustering.\n",
    "\n",
    "### Pasos\n",
    "\n",
    "#### Reducción de Dimensionalidad con UMAP\n",
    "\n",
    "- **Objetivo**: Visualizar los datos en un espacio de menor dimensión para identificar patrones y estructuras subyacentes.\n",
    "- **Método**: Aplicar UMAP (Uniform Manifold Approximation and Projection) para reducir la dimensionalidad de los datos a 2D o 3D.\n",
    "\n",
    "#### Identificación de Subgrupos con KMeans\n",
    "\n",
    "- **Objetivo**: Identificar subgrupos dentro de los datos reducidos dimensionalmente.\n",
    "- **Método**: Aplicar KMeans en los datos reducidos por UMAP para identificar clusters o subgrupos.\n",
    "\n",
    "#### Añadir Subgrupos al Dataset Original\n",
    "\n",
    "- **Objetivo**: Enriquecer el dataset original añadiendo una nueva columna que indique el subgrupo al que pertenece cada punto.\n",
    "- **Método**: Agregar los subgrupos identificados por KMeans como una nueva columna en el dataset original.\n",
    "\n",
    "## Beneficios de Esta Estrategia\n",
    "\n",
    "- **Mayor Información**: Añadir subgrupos proporciona información adicional que puede ayudar a los modelos supervisados a capturar mejor las estructuras subyacentes en los datos.\n",
    "- **Aumento de Datos**: Aunque no se aumentan las filas, se enriquece el dataset con nueva información, lo que puede mejorar el rendimiento de los modelos.\n",
    "- **Mejor Visualización**: UMAP permite visualizar los datos en un espacio reducido, facilitando la identificación de patrones y anomalías.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDZ3tcuNzOZL"
   },
   "source": [
    "Reducción de Dimensionalidad con UMAP\n",
    "\n",
    "Primero, realizaremos la reducción de dimensionalidad utilizando UMAP para visualizar los datos y facilitar la detección de patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1719477186321,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "hEMbNMm7zOZL"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seleccionar características relevantes (excluyendo columnas no numéricas si es necesario)\n",
    "features = df_combined_transformed.columns[1:-2]  # Asumiendo que las últimas dos columnas son 'Tumor type' y 'AJCC Stage'\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = df_combined_transformed.drop(columns=['Tumor type'])\n",
    "y = df_combined_transformed['Tumor type']\n",
    "\n",
    "# Aplicar UMAP\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_reducer.fit_transform(X)\n",
    "\n",
    "# Visualización de UMAP\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=df_combined_transformed['Tumor type'], palette='viridis', legend='full')\n",
    "plt.title('UMAP of Cancer Types')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend(title='Tumor Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqpFWOivzOZM"
   },
   "source": [
    "Clustering con K-Means\n",
    "\n",
    "Aplicamos K-Means para identificar subgrupos en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1719477186321,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Lkct6EkfzOZM"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Aplicar K-Means para agrupar los datos\n",
    "kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_umap)\n",
    "\n",
    "# Visualización de Clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=clusters, palette='viridis', legend='full')\n",
    "plt.title('K-Means Clustering of UMAP Components')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1719477186321,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "hqNmhoTFzOZM"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from ctgan import CTGANSynthesizer\n",
    "\n",
    "# Añadir clusters como nueva característica\n",
    "df_combined_transformed['Cluster'] = clusters\n",
    "\n",
    "# Seleccionar características y variable objetivo\n",
    "X = df_combined_transformed[features.tolist() + ['Cluster']]\n",
    "y = df_combined_transformed['Tumor type']\n",
    "\n",
    "# División del dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Aplicar CTGAN para el sobremuestreo de la clase minoritaria\n",
    "ctgan = CTGAN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ctgan.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1719477186321,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "d2bTjiS9zOZM"
   },
   "outputs": [],
   "source": [
    "# Entrenar un clasificador de Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Realizar predicciones y evaluar el modelo\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on5tRpEMzOZM"
   },
   "source": [
    "### Posibles Razones del Desempeño\n",
    "\n",
    "#### Incoherencia en los Clusters\n",
    "\n",
    "- Es posible que los clusters generados no sean lo suficientemente informativos o consistentes con las etiquetas originales. Esto puede agregar ruido en lugar de valor al modelo.\n",
    "\n",
    "#### Sobreamuestreo con SMOTE\n",
    "\n",
    "- Aunque SMOTE ayuda a balancear las clases, también puede introducir ruido si los puntos sintéticos no representan bien la distribución real de la clase minoritaria.\n",
    "\n",
    "#### Complejidad del Modelo\n",
    "\n",
    "- La inclusión de clusters puede haber aumentado la complejidad del modelo sin proporcionar una ganancia de información significativa, lo que puede llevar a un sobreajuste o subajuste.\n",
    "\n",
    "### Próximos Pasos\n",
    "\n",
    "Dado que el rendimiento del modelo no ha mejorado significativamente, seguiremos explorando otras técnicas para mejorar nuestro dataset y el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1719477186321,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "IsRwhMOVzOZN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from ctgan import CTGANSynthesizer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Añadir clusters como nueva característica\n",
    "df_combined_transformed['Cluster'] = clusters\n",
    "\n",
    "# Seleccionar características y variable objetivo\n",
    "features = df_combined_transformed.columns[1:-2]\n",
    "X = df_combined_transformed[features.tolist() + ['Cluster']]\n",
    "y = df_combined_transformed['Tumor type']\n",
    "\n",
    "# División del dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Aplicar CTGAN para el sobremuestreo de la clase minoritaria\n",
    "ctgan = CTGAN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ctgan.fit_resample(X_train, y_train)\n",
    "\n",
    "# Definir los modelos individuales\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "lgbm_clf = lgb.LGBMClassifier(random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Definir el Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_clf),\n",
    "        ('gb', gb_clf),\n",
    "        ('lgbm', lgbm_clf),\n",
    "        ('xgb', xgb_clf)\n",
    "    ],\n",
    "    voting='soft'  # 'soft' uses predicted probabilities\n",
    ")\n",
    "\n",
    "# Entrenar el Voting Classifier\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Realizar predicciones y evaluar el modelo\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-z6CmOtzOZN"
   },
   "source": [
    "## Conclusiones y Próximos Pasos\n",
    "\n",
    "El uso de un ensamble de modelos ha demostrado ser efectivo en mejorar el rendimiento del clasificador en nuestro dataset, incluso después de la incorporación de clusters como nuevas características. Este enfoque ha permitido obtener una mejor precisión y un equilibrio adecuado entre precisión y recall en la mayoría de las clases.\n",
    "\n",
    "### Próximos Pasos\n",
    "\n",
    "#### Optimización Adicional\n",
    "\n",
    "- Realizar una optimización de hiperparámetros más exhaustiva para cada modelo dentro del ensamble y para el Voting Classifier en su conjunto. (Lo dejaremos para mas tarde si tenemos tiempo debido a la dificultad de computo)\n",
    "\n",
    "#### Manejo de Clases Minoritarias\n",
    "\n",
    "- Implementar técnicas adicionales para mejorar el rendimiento en las clases minoritarias, como ajuste de pesos de las clases en los modelos o uso de técnicas de resampling más sofisticadas. (Intentaremos centrarnos en esto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1PVN8f6zOZN"
   },
   "source": [
    "# Uso de GANs para Manejar Clases Minoritarias en un Dataset Pequeño\n",
    "\n",
    "Debido a que nuestro dataset cuenta con solo 1005 filas, hemos decidido utilizar Generative Adversarial Networks (GANs) para generar datos sintéticos y así abordar el problema de las clases minoritarias. Aquí explicamos en detalle lo que estamos haciendo, para qué sirven los GANs y por qué es importante en nuestro caso.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Generar datos sintéticos para las clases minoritarias en nuestro dataset para mejorar el balance de clases y proporcionar más datos para el entrenamiento de nuestros modelos supervisados.\n",
    "\n",
    "## Explicación de GANs y su Importancia\n",
    "\n",
    "### ¿Qué son los GANs?\n",
    "\n",
    "Los Generative Adversarial Networks (GANs) son un tipo de red neuronal compuesta por dos modelos:\n",
    "\n",
    "- **Generador**: Crea datos sintéticos a partir de ruido aleatorio.\n",
    "- **Discriminador**: Distingue entre los datos reales y los datos sintéticos generados.\n",
    "\n",
    "Estos dos modelos se entrenan de manera conjunta y competitiva: el generador intenta engañar al discriminador creando datos sintéticos realistas, mientras que el discriminador intenta mejorar su capacidad para diferenciar entre datos reales y sintéticos.\n",
    "\n",
    "### ¿Por qué usar GANs en nuestro Dataset?\n",
    "\n",
    "1. **Incremento de Datos**: Nuestro dataset original es pequeño (1005 filas). Los GANs nos permiten generar datos sintéticos adicionales que pueden enriquecer nuestro dataset y mejorar el rendimiento de los modelos supervisados.\n",
    "2. **Manejo de Clases Minoritarias**: Algunas clases en nuestro dataset están subrepresentadas. Los GANs pueden generar ejemplos sintéticos de estas clases minoritarias, ayudando a balancear el dataset y a mejorar la capacidad del modelo para aprender y generalizar sobre estas clases.\n",
    "3. **Mejora del Modelo**: Al proporcionar más ejemplos para las clases minoritarias, esperamos mejorar la precisión y el recall de nuestro modelo en estas clases, reduciendo el sesgo y la varianza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1719477186321,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "V2TeoFR7zOZN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ctgan import CTGANSynthesizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Seleccionar solo las características importantes\n",
    "important_features = [\n",
    "    'sFas (pg/ml)', 'sHER2/sEGFR2/sErbB2 (pg/ml)', 'CA 15-3 (U/ml)', 'CA19-9 (U/ml)', 'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)', 'TGFa (pg/ml)', 'Sex_Male', 'Leptin (pg/ml)', 'IL-8 (pg/ml)', 'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)', 'GDF15 (ng/ml)', 'Prolactin (pg/ml)', 'HGF (pg/ml)', 'CD44 (ng/ml)', 'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)', 'TIMP-1 (pg/ml)', 'HE4 (pg/ml)', 'Cluster'\n",
    "]\n",
    "\n",
    "X = df_combined_transformed[important_features]\n",
    "\n",
    "# Definir el generador\n",
    "def build_generator(latent_dim, num_features):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=latent_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(num_features, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Definir el discriminador\n",
    "def build_discriminator(num_features):\n",
    "    model = Sequential([\n",
    "        Dense(512, input_shape=(num_features,)),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Construir el GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential([generator, discriminator])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Función para entrenar el GAN\n",
    "def train_gan(generator, discriminator, gan, data, latent_dim, epochs=1000, batch_size=128):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        # Entrenar el discriminador\n",
    "        idx = np.random.randint(0, data.shape[0], half_batch)\n",
    "        real_samples = data[idx]\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        fake_samples = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Entrenar el generador\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        # Imprimir el progreso\n",
    "        if epoch % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}] [Elapsed Time: {elapsed_time:.2f}s]\")\n",
    "\n",
    "# Generar datos sintéticos para cada clase minoritaria\n",
    "latent_dim = 100\n",
    "num_features = X.shape[1]\n",
    "synthetic_data = []\n",
    "\n",
    "for tumor_type in [2, 3, 4, 5, 6, 7]:\n",
    "    class_data = X[df_combined_transformed['Tumor type'] == tumor_type].values\n",
    "\n",
    "    # Crear y entrenar el GAN\n",
    "    generator = build_generator(latent_dim, num_features)\n",
    "    discriminator = build_discriminator(num_features)\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    train_gan(generator, discriminator, gan, class_data, latent_dim)\n",
    "\n",
    "    # Generar nuevos datos sintéticos\n",
    "    noise = np.random.normal(0, 1, (class_data.shape[0], latent_dim))\n",
    "    synthetic_class_data = generator.predict(noise)\n",
    "\n",
    "    synthetic_class_df = pd.DataFrame(synthetic_class_data, columns=important_features)\n",
    "    synthetic_class_df['Tumor type'] = tumor_type\n",
    "\n",
    "    synthetic_data.append(synthetic_class_df)\n",
    "\n",
    "# Combinar todos los datos sintéticos generados\n",
    "synthetic_df = pd.concat(synthetic_data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PypbHnACzOZO"
   },
   "source": [
    "# Estrategia para Manejar Clases Minoritarias y Datos Limitados en el Trabajo de Fin de Master\n",
    "\n",
    "Nuestro principal problema y dificultad en este trabajo de fin de grado son los pocos datos disponibles (solo 1005 filas) y el desbalanceo de la variable objetivo. Para abordar estos desafíos, hemos seguido una estrategia que combina la generación de datos sintéticos utilizando GANs y el sobremuestreo con SMOTE.\n",
    "\n",
    "## Aplicación de Técnicas Complementarias\n",
    "\n",
    "Además de la generación de datos sintéticos y el uso de SMOTE, vamos a aplicar todas las técnicas que nos han generado buenos resultados juntas para poder tener así el mejor ajuste posible. Esto incluye el uso de un ensamble de modelos (Random Forest, Gradient Boosting, LightGBM y XGBoost) en un Voting Classifier, lo que nos permite aprovechar las fortalezas de cada modelo individual para mejorar el rendimiento general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1719477186322,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "iSs2gUyNzOZO"
   },
   "outputs": [],
   "source": [
    "# Separar características y etiquetas de los datos reales\n",
    "X_real = df_combined_transformed[important_features]\n",
    "y_real = df_combined_transformed['Tumor type']\n",
    "\n",
    "# Combinar datos sintéticos y reales\n",
    "X_combined = pd.concat([X_real, synthetic_df[important_features]], axis=0)\n",
    "y_combined = pd.concat([y_real, synthetic_df['Tumor type']], axis=0)\n",
    "\n",
    "# Aplicar CTGAN para sobremuestrear las clases minoritarias en el conjunto combinado\n",
    "ctgan = CTGAN(random_state=42, k_neighbors=5)\n",
    "X_resampled, y_resampled = ctgan.fit_resample(X_combined, y_combined)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Definir los modelos individuales con regularización\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "lgbm_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Definir el Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_clf),\n",
    "        ('gb', gb_clf),\n",
    "        ('lgbm', lgbm_clf),\n",
    "        ('xgb', xgb_clf)\n",
    "    ],\n",
    "    voting='soft'  # 'soft' uses predicted probabilities\n",
    ")\n",
    "\n",
    "# Validación cruzada estratificada\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Mean Cross-Validation Accuracy: {cv_scores.mean()}')\n",
    "\n",
    "# Entrenar el Voting Classifier con todos los datos resampleados\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones y evaluar el modelo en el conjunto de prueba\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbbB443rzOZO"
   },
   "source": [
    "### Resultados en el Conjunto de Prueba\n",
    "\n",
    "|            | Precision | Recall | F1-Score | Support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "| **Clase 0** | 0.81      | 0.95   | 0.87     | 77      |\n",
    "| **Clase 1** | 0.83      | 0.84   | 0.84     | 77      |\n",
    "| **Clase 2** | 0.97      | 0.91   | 0.94     | 78      |\n",
    "| **Clase 3** | 0.99      | 0.97   | 0.98     | 78      |\n",
    "| **Clase 4** | 0.93      | 0.82   | 0.87     | 78      |\n",
    "| **Clase 5** | 0.97      | 1.00   | 0.99     | 77      |\n",
    "| **Clase 6** | 0.97      | 0.96   | 0.97     | 78      |\n",
    "| **Clase 7** | 0.94      | 0.94   | 0.94     | 78      |\n",
    "| **Accuracy** |           |        | 0.92     | 621     |\n",
    "| **Macro Avg** | 0.93    | 0.92   | 0.92     | 621     |\n",
    "| **Weighted Avg** | 0.93 | 0.92   | 0.92     | 621     |\n",
    "\n",
    "\n",
    "- **Validación Cruzada**: Hemos obtenido una precisión media de validación cruzada del 91.02%, lo que indica una buena generalización del modelo en diferentes subconjuntos del dataset.\n",
    "- **Conjunto de Prueba**: La precisión global del 92% y las métricas detalladas por clase muestran un buen equilibrio entre precisión y recall para la mayoría de las clases, especialmente las minoritarias, lo que refleja la efectividad de nuestra estrategia.\n",
    "\n",
    "El incremento ha sido el más notable, concluyendo que los datos sintéticos generados con GANs es lo mejor para datos clínicos de los que no se puede obtener muchos datos. Es la mejor manera de abordar este tipo de problemática.\n",
    "\n",
    "Aún se puede mejorar más estas métricas con la optimización de hiperparámetros (hypertuning) del Voting Classifier. (Esto si que merece la pena, hay que dejarlo mucho tiempo pero podemos tener un modelo casi perfecto con esto, Deberiamos hacerlo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co07E-YDzOZO"
   },
   "source": [
    "## Generar las curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1719477186322,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "NXzU9VgfzOZO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Binarizar las etiquetas para ROC\n",
    "y_train_bin = label_binarize(y_train, classes=np.unique(y_combined))\n",
    "y_test_bin = label_binarize(y_test, classes=np.unique(y_combined))\n",
    "n_classes = y_train_bin.shape[1]\n",
    "\n",
    "# Definir el clasificador OneVsRest\n",
    "classifier = OneVsRestClassifier(voting_clf)\n",
    "y_score = classifier.fit(X_train, y_train_bin).predict_proba(X_test)\n",
    "\n",
    "# Calcular la curva ROC y el área bajo la curva para cada clase\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Obtener los nombres de las clases originales\n",
    "class_names = label_encoder.inverse_transform(np.unique(y_train_encoded))\n",
    "\n",
    "# Graficar las curvas ROC\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Each Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87cL-xCDzOZP"
   },
   "source": [
    "### Conclusión\n",
    "\n",
    "El modelo de clasificación utilizado para distinguir entre diferentes tipos de cáncer muestra un rendimiento excelente en general, según las curvas ROC y los valores de AUC para cada clase. Las conclusiones específicas incluyen:\n",
    "\n",
    "- **Rendimiento Alto**: Todas las clases tienen un AUC superior a 0.90, lo que indica que el modelo tiene una alta capacidad para distinguir entre los diferentes tipos de cáncer.\n",
    "- **Clases con Rendimiento Perfecto o Casi Perfecto**: El modelo presenta un rendimiento perfecto para el cáncer de ovario (AUC = 1.00) y casi perfecto para el cáncer de páncreas (AUC = 0.99), lo que sugiere que el modelo puede identificar estos tipos de cáncer con muy alta precisión.\n",
    "- **Ligeras Diferencias en el Rendimiento**: Aunque todas las clases tienen un AUC alto, hay ligeras variaciones. Por ejemplo, el cáncer de hígado y el cáncer de estómago tienen AUC de 0.91 y 0.93, respectivamente, lo que indica un buen rendimiento, pero ligeramente inferior al de otros tipos de cáncer como el de mama, pulmón y páncreas.\n",
    "- **Capacidad de Discriminación**: Las curvas ROC para todas las clases están cerca de la esquina superior izquierda del gráfico, lo que refuerza la alta capacidad de discriminación del modelo para todas las clases.\n",
    "\n",
    "En resumen, el modelo es altamente eficaz para la detección y clasificación de diferentes tipos de cáncer, mostrando capacidades de discriminación muy altas y un rendimiento consistente en todos los tipos de cáncer analizados. Esto sugiere que el modelo puede ser una herramienta valiosa para la detección temprana y el diagnóstico de cáncer en un entorno clínico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GOvdoPnzOZP"
   },
   "source": [
    "Comparacion con Cancer A1DE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48Syzo48zOZP"
   },
   "source": [
    "![image.png](attachment:image.png) VS ![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck_BVgbEzOZP"
   },
   "source": [
    "### Comparación de Modelos usando ROC y AUC\n",
    "\n",
    "#### Conceptos Clave:\n",
    "1. **ROC (Receiver Operating Characteristic) Curve**: Muestra la relación entre la Tasa de Verdaderos Positivos (sensibilidad) y la Tasa de Falsos Positivos para varios umbrales de clasificación.\n",
    "2. **AUC (Área Bajo la Curva)**: Representa la capacidad del modelo para discriminar entre las clases. Un AUC de 1.0 indica un modelo perfecto, mientras que un AUC de 0.5 indica un modelo que no es mejor que una suposición aleatoria.\n",
    "\n",
    "#### Comparación:\n",
    "\n",
    "1. **Nuestro Modelo**:\n",
    "   - Las curvas ROC individuales para cada clase muestran una alta sensibilidad y especificidad, lo que indica una fuerte capacidad de discriminación.\n",
    "   - Los valores de AUC para cada clase son:\n",
    "     - Breast: 0.98\n",
    "     - Colorectum: 0.96\n",
    "     - Esophagus: 0.93\n",
    "     - Liver: 0.91\n",
    "     - Lung: 0.97\n",
    "     - Ovary: 1.00\n",
    "     - Pancreas: 0.99\n",
    "     - Stomach: 0.93\n",
    "\n",
    "2. **Modelo CancerA1DE**:\n",
    "   - El AUC promedio es de 0.83.\n",
    "   - Las curvas ROC en el modelo CancerA1DE muestran menor discriminación entre algunas combinaciones de clases.\n",
    "\n",
    "#### Interpretación:\n",
    "- **AUC Promedio**: El AUC promedio de nuestro modelo es superior al del modelo CancerA1DE (0.83), lo que sugiere que, en promedio, nuestro modelo tiene una mejor capacidad de discriminación.\n",
    "- **Curvas ROC**: Las curvas ROC de nuestro modelo están más cerca de la esquina superior izquierda, lo que indica una mejor sensibilidad y especificidad para la mayoría de las clases en comparación con el modelo CancerA1DE.\n",
    "\n",
    "#### Conclusión:\n",
    "- **Ventajas de Nuestro Modelo**:\n",
    "  - Mejores valores de AUC, indicando una mayor precisión en la clasificación.\n",
    "  - Curvas ROC más pronunciadas, indicando una mejor capacidad de discriminación.\n",
    "  \n",
    "- **Limitaciones de Comparación**:\n",
    "  - Aunque el AUC y la ROC son métricas valiosas, no son las únicas. Otras métricas como precisión, recall, F1-score, y la matriz de confusión también son importantes para evaluar la eficacia de los modelos pero no tenemos estos resultados para comparar con CancerA1DE\n",
    "\n",
    "En resumen, basado en la comparación de AUC y ROC, nuestro modelo muestra un rendimiento superior en la clasificación de los tipos de cáncer en comparación con el modelo CancerA1DE. Sin embargo, es importante considerar una evaluación más amplia con diferentes métricas para una comparación exhaustiva.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ESMZcOZzOZP"
   },
   "source": [
    "# Creacion de AUC Promedio Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1719477186322,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "rkkBMAN9zOZP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Binarizar las etiquetas para ROC\n",
    "y_combined_bin = label_binarize(y_combined, classes=np.unique(y_combined))\n",
    "n_classes = y_combined_bin.shape[1]\n",
    "\n",
    "# Definir el clasificador OneVsRest\n",
    "classifier = OneVsRestClassifier(voting_clf)\n",
    "\n",
    "# 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Para almacenar fpr, tpr y roc_auc para cada clase\n",
    "fpr = {i: [] for i in range(n_classes)}\n",
    "tpr = {i: [] for i in range(n_classes)}\n",
    "roc_auc = {i: [] for i in range(n_classes)}\n",
    "\n",
    "for train, test in cv.split(X_combined, y_combined):\n",
    "    X_train, X_test = X_combined.iloc[train], X_combined.iloc[test]\n",
    "    y_train, y_test = y_combined_bin[train], y_combined_bin[test]\n",
    "\n",
    "    y_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr_fold, tpr_fold, _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc_fold = auc(fpr_fold, tpr_fold)\n",
    "\n",
    "        fpr[i].append(fpr_fold)\n",
    "        tpr[i].append(tpr_fold)\n",
    "        roc_auc[i].append(roc_auc_fold)\n",
    "\n",
    "        # Interpolación para promediar las curvas\n",
    "        interp_tpr = np.interp(mean_fpr, fpr_fold, tpr_fold)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(roc_auc_fold)\n",
    "\n",
    "# Calcular los valores medios y las bandas de confianza\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "# Obtener los nombres de las clases originales\n",
    "class_names = label_encoder.inverse_transform(np.unique(y_train_encoded))\n",
    "\n",
    "# Graficar las curvas ROC para cada clase\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    mean_fpr_i = np.linspace(0, 1, 100)\n",
    "    mean_tpr_i = np.mean([np.interp(mean_fpr_i, fpr_fold, tpr_fold) for fpr_fold, tpr_fold in zip(fpr[i], tpr[i])], axis=0)\n",
    "    mean_tpr_i[-1] = 1.0\n",
    "    mean_auc_i = auc(mean_fpr_i, mean_tpr_i)\n",
    "    std_auc_i = np.std(roc_auc[i])\n",
    "    plt.plot(mean_fpr_i, mean_tpr_i, label=f'{class_names[i]} (area = {mean_auc_i:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Each Class with 10-Fold Cross-Validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calcular AUC promedio global ponderado por la cantidad de muestras de cada clase\n",
    "class_counts = y_combined_bin.sum(axis=0)\n",
    "total_samples = len(y_combined)\n",
    "weighted_auc = np.dot([np.mean(roc_auc[i]) for i in range(n_classes)], class_counts) / total_samples\n",
    "\n",
    "print(f'AUC promedio global ponderado: {weighted_auc:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umxpGK53zOZQ"
   },
   "source": [
    "Lo que debemos hacer es centrarnos aún mas en la generacion de datos sinteticos con GANs, Le he dado 1000 epocas, podriamos probar con 3000 epocas y luego mejorar aún mas nuestro voting classifier.\n",
    "Esto puede que haga que nuestro modelo mejores (Hay que ver cuanto podemos mejora esto para que quede perfecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmDH3M2qzOZQ"
   },
   "source": [
    "Tambien podemos platear un nuevo feature que es el ADN libre en sangre, ya que cuando hicmos el EDA, pudimos ver que influia en el tipo de cancer, teniendo valores altos para alguno de ellos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zycbbu1kzOZQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
